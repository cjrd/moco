{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE is based of fastautoagument code here \n",
    "\n",
    "# https://github.com/kakaobrain/fast-autoaugment/blob/master/FastAutoAugment/search.py\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import ray \n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "from ray.tune import register_trainable, run_experiments\n",
    "import wandb\n",
    "import argparse\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/userdata/smetzger/all_deepul_files/deepul_proj/moco/\")\n",
    "import moco.loader\n",
    "import moco.builder\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "import os\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from imagenet import ImageNet, SubsetSampler # Kakao brain stuff. \n",
    "from torch.utils.data import SubsetRandomSampler, Sampler, Subset, ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "# FOR DEBUG\n",
    "class Args:\n",
    "    base = 'moco_rrc'  # Moco or moco_rrc\n",
    "\n",
    "    # if base == 'moco': \n",
    "    #     checkpoints = ['fxrZE', 'lJu2W', 'rdEIg', 'esdq2' ,'vnhKs'] # Ordered KFOLDS order. Make this nicer.\n",
    "    \n",
    "    # else: \n",
    "    # checkpoints = ['6bV5F', 'Pr3wZ', '43Hdo','h18r9', 'TcXI9']\n",
    "\n",
    "    checkpoints = ['RdNs9', 'USw8a', 'RwAwX', 'UAjDH', 'NywPV'] # SVHN rrc checkpoint names\n",
    "    checkpoint_fp = '/userdata/smetzger/all_deepul_files/ckpts'\n",
    "    data = '/userdata/smetzger/data/cifar_10/'\n",
    "    \n",
    "    # Some args for FAA. \n",
    "    num_op = 2\n",
    "    num_policy=5\n",
    "    num_search = 200\n",
    "    dataid = 'imagenet'\n",
    "    cv_ratio=1.0\n",
    "    smoke_test=False\n",
    "    resume=False\n",
    "    arch = 'resnet50'\n",
    "    distributed=False\n",
    "    loss = 'icl_and_rotation'# one of rotation, supervised, icl, icl_and_rotation.\n",
    "#     base = 'moco_rrc' # Name for what we are saving our training runs as.\n",
    "\n",
    "    # Moco args. \n",
    "    moco_k = 65536\n",
    "    moco_m = 0.999\n",
    "    moco_t = 0.2\n",
    "    \n",
    "    \n",
    "    # Whether or not to use the MLP for mocov2\n",
    "    mlp = True\n",
    "    \n",
    "    # Model input args for building the model head. \n",
    "    nomoco = False\n",
    "    rotnet = False\n",
    "    \n",
    "    moco_dim = 128\n",
    "    dim_mlp = 2048\n",
    "    \n",
    "    policy_dir = '/userdata/smetzger/all_deepul_files/policies'\n",
    "    \n",
    "    # Remember we are trying to max negative loss, so a negative here\n",
    "    # is like maximizing, a positive here is like minimizing. \n",
    "    loss_weights = {'rotation': 1/27.4, 'icl': -1/6.6, 'supervised':1} # weight for ICL, and ROTATION. Divide by mean.\n",
    "    # NOTE: WHY IS ROTATION SO MUCH HIGHER FOR IMGNET? \n",
    "\n",
    "    \n",
    "args=Args()\n",
    "print('args', args)\n",
    "\n",
    "import random\n",
    "\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.transforms import Compose\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "random_mirror = True\n",
    "from self_aug.autoaug_scripts import augment_list, Augmentation, Accumulator\n",
    "\n",
    "# Define how we load our dataloaders. \n",
    "_CIFAR_MEAN, _CIFAR_STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "def get_dataloaders(augmentations, batch=1024, kfold=0, loss_type='icl', get_train=False):\n",
    "\n",
    "    \"\"\"\n",
    "    input: augmentations: the list of the augmentations you want applied to the data. \n",
    "    batch = batchsize, \n",
    "    kfold, which fold you want to look at (0, 1,2 3, or 4)\n",
    "    get_train, whether or not you want the train data. Use this when loading the data to train linear classifiers, \n",
    "    slash when you're loading the final classifier. \n",
    "    \"\"\"\n",
    "    if args.dataid == \"imagenet\":\n",
    "        # train_dataset = datasets.ImageFolder(\n",
    "        #     traindir,\n",
    "        #     transformations)\n",
    "        rrc_sz = 224\n",
    "        mu =[0.485, 0.456, 0.406]\n",
    "        std=[0.229, 0.224, 0.225] \n",
    "\n",
    "    else: \n",
    "        print('USING RRC = 28')\n",
    "        rrc_sz = 28\n",
    "        mu = _CIFAR_STD\n",
    "        std = _CIFAR_MEAN\n",
    "\n",
    "   \n",
    "\n",
    "    # THe default training transforms we use when training the CIFAR10 network. \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(rrc_sz, scale=(0.2, 1.)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mu, std),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # If we're evaluating ICL, its only fair to do so with the ICL augmentations\n",
    "    if loss_type == \"icl\": \n",
    "        \n",
    "        random_resized_crop = transforms.RandomResizedCrop(rrc_sz, scale=(0.2, 1.))\n",
    "       \n",
    "        if 'rrc' in args.base:\n",
    "            transform_train = transforms.Compose([\n",
    "                random_resized_crop, \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mu, std)\n",
    "            ])\n",
    "        \n",
    "        else: \n",
    "            print(\"ON TOP OF OG MOCOV2. CATASTROPHIC!\")\n",
    "            transform_train = transforms.Compose([\n",
    "            random_resized_crop,\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mu, std),\n",
    "            ])\n",
    "\n",
    "    # Insert the new transforms in to the training transforms. \n",
    "    transform_train.transforms.insert(0, Augmentation(augmentations))\n",
    "    # Use the twocrops transform. \n",
    "    if loss_type == \"icl\": \n",
    "        transform_train = moco.loader.TwoCropsTransform(transform_train)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Support for the following dataset is not yet implemented: {}\".format(args.dataid))\n",
    "\n",
    "    if get_train: \n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.data,\n",
    "                                                     transform=transform_train,\n",
    "                                                     download=True)\n",
    "\n",
    "    # In FAA They use Train Transform as well.\n",
    "    if args.dataid == \"cifar10\":\n",
    "        val_dataset = torchvision.datasets.CIFAR10(args.data, transform=transform_train, \n",
    "            download=True)\n",
    "\n",
    "    elif args.dataid == \"svhn\": \n",
    "        val_dataset = torchvision.datasets.SVHN(args.data, transform=transform_train, \n",
    "            download=True)\n",
    "\n",
    "    elif args.dataid == \"imagenet\":\n",
    "        valid_dataset = ImageNet(root=args.data, transform=transform_train)\n",
    "        train_idx = np.arange(len(valid_dataset))\n",
    "        np.random.seed(1337)\n",
    "        np.random.shuffle(train_idx)\n",
    "        train_idx = train_idx[:50000]\n",
    "\n",
    "        kfold = args.kfold\n",
    "\n",
    "        print('KFOLD BEING USED', kfold)\n",
    "        subset = np.arange(kfold*10000, (kfold+1)*10000)\n",
    "        print('start', 'end', kfold*10000, (kfold+1)*10000)\n",
    "        valid_idx = train_idx[subset]\n",
    "        train_idx = np.delete(train_idx, subset)\n",
    "\n",
    "        print('first val_idx', valid_idx[:10])\n",
    "        print('firstidx', valid_idx[:10])\n",
    "\n",
    "        valid_dataset = Subset(valid_dataset, valid_idx)\n",
    "        val_dataset = valid_dataset\n",
    "    \n",
    "\n",
    "    if get_train and not args.dataid ==\"imagenet\": \n",
    "        torch.manual_seed(1337)\n",
    "        lengths = [len(train_dataset)//5]*5\n",
    "        folds = torch.utils.data.random_split(train_dataset, lengths)\n",
    "        folds.pop(kfold)\n",
    "        train_dataset = torch.utils.data.ConcatDataset(folds)\n",
    "\n",
    "\n",
    "        torch.manual_seed(1337)\n",
    "        lengths = [len(val_dataset)//5]*5\n",
    "        lengths[-1] = int(lengths[-1] + (len(val_dataset)-np.sum(lengths)))\n",
    "        # print(lengths)\n",
    "        folds = torch.utils.data.random_split(val_dataset, lengths)\n",
    "        val_dataset = folds[kfold]\n",
    "\n",
    "    if get_train: \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
    "            num_workers=8, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "    if not args.dataid == \"imagenet\":\n",
    "        val_loader= torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=batch, shuffle=True,\n",
    "            num_workers=4, pin_memory=True, drop_last=False, \n",
    "            sampler=None\n",
    "        )\n",
    "    else: \n",
    "        val_loader= torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=batch, shuffle=True,\n",
    "            num_workers=4, pin_memory=True, drop_last=False, \n",
    "            sampler=valid_sampler\n",
    "        )\n",
    "\n",
    "\n",
    "    if not get_train: \n",
    "        train_loader = None\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "#  Go from the hyperopt parameters to the actual policy. \n",
    "def policy_decoder(augment, num_policy, num_op):\n",
    "    op_list = augment_list(False)\n",
    "    policies = []\n",
    "    for i in range(num_policy):\n",
    "        ops = []\n",
    "        for j in range(num_op):\n",
    "            op_idx = augment['policy_%d_%d' % (i, j)]\n",
    "            op_prob = augment['prob_%d_%d' % (i, j)]\n",
    "            op_level = augment['level_%d_%d' % (i, j)]\n",
    "            ops.append((op_list[op_idx][0].__name__, op_prob, op_level))\n",
    "        policies.append(ops)\n",
    "    return policies\n",
    "\n",
    "\n",
    "\n",
    "fuckthis = ['3hp7c', 'XTusE', 'Q6QAS', 'YH7ck', 'bNNTv'] # Because I couldn't delete old runs. \n",
    "def find_model(name, fold, epochs, basepath=\"/userdata/smetzger/all_deepul_files/ckpts\"):\n",
    "    \"\"\"\n",
    "    name = model name\n",
    "    fold = which fold of the data to find. \n",
    "    epochs = how many epochs to load the checkpoint at (e.g. 750)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    path_list = []\n",
    "\n",
    "    for file in os.listdir(basepath):\n",
    "        if name in str(file):\n",
    "            if str(file).endswith(str(epochs-1) + '.tar'): \n",
    "                if str(file)[:5] in fuckthis:\n",
    "                    if 'fold_%d' %(fold) in file: \n",
    "                        return (os.path.join(basepath, file))\n",
    "            \n",
    "    print(\"COULDNT FIND MODEL\")\n",
    "    assert True==False # just throw and error. \n",
    "\n",
    "def load_model(cv_fold, loss_type): \n",
    "    \n",
    "    model = models.__dict__[args.arch]()\n",
    "    \n",
    "    if args.dataid ==\"cifar10\" or args.dataid == \"svhn\":\n",
    "    # use the layer the SIMCLR authors used for cifar10 input conv, checked all padding/strides too.\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1,1), padding=(1,1), bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    if args.dataid == \"cifar10\" or args.dataid == \"svhn\":\n",
    "\n",
    "        if loss_type == 'supervised':\n",
    "            model.fc = torch.nn.Linear(model.fc.in_features, 10) # note this is for cifar 10.\n",
    "            \n",
    "    if loss_type =='rotation': \n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 4)\n",
    "\n",
    "    # Load the checkpoints. \n",
    "    if loss_type == 'supervised': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                \"{}_lincls_best.tar\".format(args.checkpoints[cv_fold]))\n",
    "    elif loss_type == 'rotation': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                 \"{}_lincls_best_rotation.tar\".format(args.checkpoints[cv_fold]))\n",
    "\n",
    "    \n",
    "    elif loss_type == 'icl': \n",
    "#         print('ICL')\n",
    "        heads = {}\n",
    "        if not args.nomoco:\n",
    "            heads[\"moco\"] = {\n",
    "            \"num_classes\": args.moco_dim\n",
    "        }\n",
    "\n",
    "        model = moco.builder.MoCo(\n",
    "            models.__dict__[args.arch],\n",
    "            K=args.moco_k, m=args.moco_m, T=args.moco_t, mlp=args.mlp, dataid=args.dataid,\n",
    "            multitask_heads=heads\n",
    "        )\n",
    "        savefile = find_model(args.checkpoints[cv_fold], cv_fold)\n",
    "        print(savefile)\n",
    "        \n",
    "    ckpt = torch.load(savefile, map_location=\"cpu\")\n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith('module'):\n",
    "            state_dict[k[len(\"module.\"):]] = state_dict[k] \n",
    "            del state_dict[k]\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model  \n",
    "\n",
    "def rotate_images(images):\n",
    "    nimages = images.shape[0]\n",
    "    n_rot_images = 4*nimages\n",
    "\n",
    "    # rotate images all 4 ways at once\n",
    "    rotated_images = torch.zeros([n_rot_images, images.shape[1], images.shape[2], images.shape[3]]).cuda()\n",
    "    rot_classes = torch.zeros([n_rot_images]).long().cuda()\n",
    "\n",
    "    rotated_images[:nimages] = images\n",
    "    # rotate 90\n",
    "    rotated_images[nimages:2*nimages] = images.flip(3).transpose(2,3)\n",
    "    rot_classes[nimages:2*nimages] = 1\n",
    "    # rotate 180\n",
    "    rotated_images[2*nimages:3*nimages] = images.flip(3).flip(2)\n",
    "    rot_classes[2*nimages:3*nimages] = 2\n",
    "    # rotate 270\n",
    "    rotated_images[3*nimages:4*nimages] = images.transpose(2,3).flip(3)\n",
    "    rot_classes[3*nimages:4*nimages] = 3\n",
    "\n",
    "    return rotated_images, rot_classes\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def eval_augmentations(config): \n",
    "    augment = config\n",
    "    print('called', augment)\n",
    "\n",
    "    # If we have ICL and rotation, loop over the two because of the different way we evaluate\n",
    "    # Either on top of the mocov2 augs, or on top of the lincls train augs. \n",
    "    if args.loss == 'icl_and_rotation':\n",
    "        losses = ['icl', 'rotation']\n",
    "    else: \n",
    "        losses = [args.loss]\n",
    "\n",
    "    metrics = Accumulator()\n",
    "    \n",
    "    for loss_type in losses: \n",
    "      \n",
    "        print(loss_type)\n",
    "        augmentations = policy_decoder(augment, augment['num_policy'], augment['num_op'])\n",
    "        \n",
    "        fold = augment['cv_fold']\n",
    "\n",
    "        # Load the model, either the rotnet/lincls head, or the full MoCo model.\n",
    "        model = load_model(fold, loss_type).cuda()\n",
    "\n",
    "        model.eval()\n",
    "        loaders = []\n",
    "\n",
    "        runs = 5\n",
    "        if args.dataid == 'imagenet': \n",
    "            runs = 2 # just cut down on time.\n",
    "        for _ in range(runs): # there was a todo in the original moco code. basically we just load 5 loaders.\n",
    "            _, validloader = get_dataloaders(augmentations, 128, kfold=fold, loss_type=loss_type)\n",
    "            loaders.append(iter(validloader))\n",
    "            del _\n",
    "\n",
    "     \n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        try: \n",
    "            with torch.no_grad(): \n",
    "                while True: \n",
    "                    losses = []\n",
    "                    corrects = []\n",
    "\n",
    "                    for loader in loaders:\n",
    "\n",
    "                        if not loss_type == 'icl':\n",
    "\n",
    "                            data, label = next(loader)\n",
    "                            data = data.cuda()\n",
    "                            label = label.cuda()\n",
    "\n",
    "                            if loss_type == 'supervised':\n",
    "                                pred = model(data)\n",
    "\n",
    "                            if loss_type ==\"rotation\":\n",
    "                                rotated_images, label = rotate_images(data)\n",
    "                                pred = model(rotated_images)  \n",
    "\n",
    "                        else: \n",
    "\n",
    "                            images, _ = next(loader)\n",
    "                            images[0] = images[0].cuda(non_blocking=True)\n",
    "                            images[1] = images[1].cuda(non_blocking=True)\n",
    "                            pred, label =model(head=\"moco\", im_q=images[0], im_k=images[1], evaluate=True)\n",
    "                            acc = accuracy(pred, label)\n",
    "\n",
    "                        loss = loss_fn(pred, label)\n",
    "                        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                        _, pred = pred.topk(1, 1, True, True)\n",
    "                        pred = pred.t()\n",
    "                        correct = pred.eq(label.view(1, -1).expand_as(pred)).detach().cpu().numpy()\n",
    "                        corrects.append(correct)\n",
    "\n",
    "                        if not loss_type == 'icl':\n",
    "                            del loss, correct, pred, data, label\n",
    "                        else: \n",
    "                            del loss, images, pred, label, correct\n",
    "\n",
    "\n",
    "\n",
    "                    losses = np.concatenate(losses)\n",
    "#                     print(losses.shape)\n",
    "#                     print('losses shape' , losses.shape) This would usually just be 5*512, \n",
    "                    losses_min = np.mean(losses) # get it so it averages out intead of taking the smallest losses. \n",
    "                    corrects = np.concatenate(corrects)\n",
    "                    corrects_max = np.max(corrects, axis=0).squeeze() # 5, 512, \n",
    "#                     print('len corrects max', len(corrects_max), 'corrects shape', corrects.shape)\n",
    "                    losses_min *= losses.shape[0]/5 # Divide by the losses we're using. \n",
    "                    \n",
    "                    if loss_type == 'rotation': \n",
    "\n",
    "                        # Scale the loss by 1/4 since there are 4x as many samples. \n",
    "                        metrics.add_dict({ \n",
    "                            'minus_loss': -.25*np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'plus_loss': np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'correct': np.sum(corrects_max)*args.loss_weights[loss_type],\n",
    "                            'cnt': len(corrects_max)})\n",
    "                        del corrects, corrects_max\n",
    "                    else: \n",
    "                        metrics.add_dict({ \n",
    "                            'minus_loss': -1*np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'plus_loss': np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'correct': np.sum(corrects_max)*args.loss_weights[loss_type],\n",
    "                            'cnt': len(corrects_max)})\n",
    "                        del corrects, corrects_max\n",
    "        except StopIteration: \n",
    "            pass\n",
    "    \n",
    "    del model\n",
    "    metrics = metrics/'cnt'\n",
    "\n",
    "    # This is what it keeps track of for hyperopting. \n",
    "    tune.track.log(top_1_valid=metrics['correct'], minus_loss=metrics['minus_loss'], plus_loss=metrics['plus_loss'])\n",
    "    return metrics['minus_loss']\n",
    "\n",
    "ops = augment_list(False) # Get the default augmentation set. \n",
    "# Define the space of our augmentations. \n",
    "space = {}\n",
    "for i in range(args.num_policy): \n",
    "    for j in range(args.num_op):\n",
    "        space['policy_%d_%d' %(i,j)]  = hp.choice('policy_%d_%d' %(i, j), list(range(0, len(ops))))\n",
    "        space['prob_%d_%d' %(i, j)] = hp.uniform('prob_%d_%d' %(i, j), 0.0, 1.0)\n",
    "        space['level_%d_%d' %(i, j)] = hp.uniform('level_%d_%d' %(i, j), 0.0, 1.0)\n",
    "\n",
    "final_policy_set = []\n",
    "    \n",
    "reward_attr = 'minus_loss' \n",
    "\n",
    "# TODO boost this. \n",
    "ray.init(num_gpus=2, ignore_reinit_error=True, \n",
    "    num_cpus=28\n",
    ")\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "cv_num = 5\n",
    "num_result_per_cv = 10\n",
    "\n",
    "for _ in range(2):  # 2 experiments from FAA. \n",
    "    for cv_fold in range(cv_num): # For the 5 folds. \n",
    "        name = \"slm_imgnet_min_max_%s_fold_%d\" %(args.dataid, cv_fold)\n",
    "        hyperopt_search=HyperOptSearch(space, \n",
    "            max_concurrent=4*20, # TODO Up this. \n",
    "            metric=reward_attr,\n",
    "            mode='max')\n",
    "\n",
    "        results = tune.run(\n",
    "            eval_augmentations, \n",
    "            name=name,\n",
    "            num_samples=200, #TODO up this.\n",
    "            resources_per_trial={\n",
    "                \"gpu\":1\n",
    "            },\n",
    "            search_alg=hyperopt_search,\n",
    "            verbose=2,\n",
    "            config = { \n",
    "                'num_op': args.num_op, \n",
    "                'num_policy': args.num_policy, \n",
    "                'cv_fold': cv_fold\n",
    "            },\n",
    "            return_trials=True,\n",
    "            stop={'training_iteration': 1},\n",
    "        )\n",
    "        results_copy = results\n",
    "        results = [x for x in results if x.last_result is not None]\n",
    "        results = sorted(results, key= lambda x: x.last_result[reward_attr], reverse=True)\n",
    "\n",
    "        for result in results[:num_result_per_cv]: \n",
    "            final_policy = policy_decoder(result.config, args.num_policy, args.num_op)\n",
    "            final_policy_set.extend(final_policy)\n",
    "\n",
    "        import pickle\n",
    "        savefp = os.path.join(args.policy_dir, str(args.dataid + '_weighted_' + args.base+'_'+args.loss+ '_' + (str(reward_attr) + '.pkl')))\n",
    "        with open(savefp, 'wb') as f: \n",
    "            pickle.dump(final_policy_set, f)\n",
    "            \n",
    "print(final_policy_set)\n",
    "\n",
    "# Start saving to a path called policies. \n",
    "import pickle\n",
    "savefp = os.path.join(args.policy_dir, str(args.dataid + '_weighted_' + args.base+'_'+args.loss+ '_' + (str(reward_attr) + '.pkl')))\n",
    "with open(savefp, 'wb') as f: \n",
    "    pickle.dump(final_policy_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
