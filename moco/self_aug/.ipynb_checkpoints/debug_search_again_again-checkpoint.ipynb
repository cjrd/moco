{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE is based of fastautoagument code here \n",
    "\n",
    "# https://github.com/kakaobrain/fast-autoaugment/blob/master/FastAutoAugment/search.py\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import ray \n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "from ray.tune import register_trainable, run_experiments\n",
    "import wandb\n",
    "import argparse\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/userdata/smetzger/all_deepul_files/deepul_proj/moco/\")\n",
    "import moco.loader\n",
    "import moco.builder\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "import os\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "# print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2, 3\"\n",
    "\n",
    "# What to use for our moco model. \n",
    "\n",
    "# argparser.add('--checkpoint_fp', type=str, help='base file path where everything is stored.')\n",
    "# argparser.add('-checkpoints' , '--list', nargs='+', help='The list of the checkpoint codes, in order by cv fold')\n",
    "# # example: search.py -checkpoints rdElg fxrZE IJu2W vnhKs esdq2\n",
    "\n",
    "# argparser.add('--data', type=str, help=\"Where the data directory is\")\n",
    "# argparser.add('--dataid', type=str, default='cifar10', help=\"imagenet or cifar\")\n",
    " \n",
    "# # Arguments for FAA.  \n",
    "# parser.add_argument('--num-op', type=int, default=2, help=\"number of operations per subpolicy.\")\n",
    "# parser.add_argument('--num-policy', type=int, default=5, help=\"number of subpolicies in each policy\")\n",
    "# parser.add_argument('--num-search', type=int, default=200, help=\"number of hyperopt iterations to run.\")\n",
    "# parser.add_argument('--smoke-test', action='store_true', help=\"quick test of our search\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# FOR DEBUG\n",
    "class Args:\n",
    "    checkpoints = ['fxrZE', 'lJu2W', 'rdEIg', 'esdq2' ,'vnhKs'] # Ordered KFOLDS order. Make this nicer.\n",
    "    checkpoint_fp = '/userdata/smetzger/all_deepul_files/ckpts'\n",
    "    data = '/userdata/smetzger/data/cifar_10/'\n",
    "    num_op = 2\n",
    "    num_policy=5\n",
    "    num_search = 200\n",
    "    dataid = 'cifar10'\n",
    "    cv_ratio=1.0\n",
    "    smoke_test=True\n",
    "    resume=False\n",
    "    arch = 'resnet50'\n",
    "    distributed=False\n",
    "    loss = 'rotation' # one of rotation, supervised, ICL, icl_and_rotation.\n",
    "args=Args()\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.transforms import Compose\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "random_mirror = True\n",
    "from autoaug_scripts import augment_list, Augmentation, Accumulator\n",
    "\n",
    "# Define how we load our dataloaders. \n",
    "_CIFAR_MEAN, _CIFAR_STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "\n",
    "def get_dataloaders(augmentations, batch=1024, kfold=0, get_train=False):\n",
    "\n",
    "    \"\"\"\n",
    "    input: augmentations: the list of the augmentations you want applied to the data. \n",
    "    batch = batchsize, \n",
    "    kfold, which fold you want to look at (0, 1,2 3, or 4)\n",
    "    get_train, whether or not you want the train data. Use this when loading the data to train linear classifiers, \n",
    "    slash when you're loading the final classifier. \n",
    "    \"\"\"\n",
    "    if args.dataid == \"imagenet\":\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            traindir,\n",
    "            transformations)\n",
    "\n",
    "        # TODO: add imagenet transforms etc. \n",
    "    elif args.dataid == \"cifar10\":\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(28, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "        transform_train.transforms.insert(0, Augmentation(augmentations))\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.CenterCrop(28),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Support for the following dataset is not yet implemented: {}\".format(args.dataid))\n",
    "\n",
    "    if get_train: \n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.data,\n",
    "                                                     transform=transform_train,\n",
    "                                                     download=True)\n",
    "\n",
    "    val_dataset = torchvision.datasets.CIFAR10(args.data, transform=transform_train, \n",
    "        download=True)\n",
    "\n",
    "    if get_train: \n",
    "        torch.manual_seed(1337)\n",
    "        lengths = [len(train_dataset)//5]*5\n",
    "        folds = torch.utils.data.random_split(train_dataset, lengths)\n",
    "        folds.pop(kfold)\n",
    "        train_dataset = torch.utils.data.ConcatDataset(folds)\n",
    "\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    lengths = [len(val_dataset)//5]*5\n",
    "    folds = torch.utils.data.random_split(val_dataset, lengths)\n",
    "    val_dataset = folds[kfold]\n",
    "\n",
    "    # if args.distributed:\n",
    "    #     # if get_train: \n",
    "    #     #     train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    #     # val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset) #TODO: is this necessary? \n",
    "    #     val_sampler = None\n",
    "\n",
    "    if get_train: \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
    "            num_workers=8, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "    val_loader= torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, drop_last=False\n",
    "    )\n",
    "\n",
    "    if not get_train: \n",
    "        train_loader = None\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Take in the augment from hyperopt and return some augmentations, in teh way that we want them. \n",
    "def policy_decoder(augment, num_policy, num_op):\n",
    "    op_list = augment_list(False)\n",
    "    policies = []\n",
    "    for i in range(num_policy):\n",
    "        ops = []\n",
    "        for j in range(num_op):\n",
    "            op_idx = augment['policy_%d_%d' % (i, j)]\n",
    "            op_prob = augment['prob_%d_%d' % (i, j)]\n",
    "            op_level = augment['level_%d_%d' % (i, j)]\n",
    "            ops.append((op_list[op_idx][0].__name__, op_prob, op_level))\n",
    "        policies.append(ops)\n",
    "    return policies\n",
    "\n",
    "def load_base_model(cv_fold): \n",
    "    \n",
    "    if args.pretrained:\n",
    "        if os.path.isfile(args.pretrained):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "            checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n",
    "\n",
    "            # rename moco pre-trained keys\n",
    "            state_dict = checkpoint['state_dict']\n",
    "            if checkpoint.get('id'):\n",
    "                # sync the ids for wandb\n",
    "                args.id = checkpoint['id']\n",
    "                name = args.id\n",
    "                wandb_resume = True\n",
    "            if checkpoint.get('name'):\n",
    "                name = checkpoint['name']\n",
    "\n",
    "            for k in list(state_dict.keys()):\n",
    "                # retain only encoder_q up to before the embedding layer\n",
    "                if k.startswith('module.model.encoder'):\n",
    "                    # remove prefix\n",
    "                    state_dict[k[len(\"module.model.encoder.\"):]] = state_dict[k]\n",
    "                # delete renamed or unused k\n",
    "                del state_dict[k]\n",
    "            args.start_epoch = 0\n",
    "            msg = model.load_state_dict(state_dict, strict=False)\n",
    "            assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
    "\n",
    "def load_model(cv_fold, loss_type): \n",
    "    model = models.__dict__[args.arch]()\n",
    "    # CIFAR 10 mod\n",
    "    \n",
    "    \n",
    "    if args.dataid ==\"cifar10\":\n",
    "    # use the layer the SIMCLR authors used for cifar10 input conv, checked all padding/strides too.\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1,1), padding=(1,1), bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    if args.dataid == \"cifar10\":\n",
    "#         print('before change', model.fc)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 10) # note this is for cifar 10.\n",
    "#         print(model.fc)\n",
    "\n",
    "\n",
    "    if loss_type == 'supervised': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                \"{}_lincls_best.tar\".format(args.checkpoints[cv_fold]))\n",
    "    elif loss_type == 'rotation': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                 \"{}_lincls_best_rotation.tar\".format(args.checkpoints[cv_fold]))\n",
    "\n",
    "    ckpt = torch.load(savefile, map_location=\"cpu\")\n",
    "    \n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    for k in list(state_dict.keys()):\n",
    "        # retain only encoder_q up to before the embedding layer\n",
    "        \n",
    "        if k.startswith('module'):\n",
    "            # remove prefix\n",
    "            \n",
    "            state_dict[k[len(\"module.\"):]] = state_dict[k]\n",
    "            del state_dict[k]\n",
    "            \n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    # Load the FC layer and append it to the end. \n",
    "\n",
    "def rotate_images(images):\n",
    "    nimages = images.shape[0]\n",
    "    n_rot_images = 4*nimages\n",
    "\n",
    "    # rotate images all 4 ways at once\n",
    "    rotated_images = torch.zeros([n_rot_images, images.shape[1], images.shape[2], images.shape[3]]).cuda()\n",
    "    rot_classes = torch.zeros([n_rot_images]).long().cuda()\n",
    "\n",
    "    rotated_images[:nimages] = images\n",
    "    # rotate 90\n",
    "    rotated_images[nimages:2*nimages] = images.flip(3).transpose(2,3)\n",
    "    rot_classes[nimages:2*nimages] = 1\n",
    "    # rotate 180\n",
    "    rotated_images[2*nimages:3*nimages] = images.flip(3).flip(2)\n",
    "    rot_classes[2*nimages:3*nimages] = 2\n",
    "    # rotate 270\n",
    "    rotated_images[3*nimages:4*nimages] = images.transpose(2,3).flip(3)\n",
    "    rot_classes[3*nimages:4*nimages] = 3\n",
    "\n",
    "    return rotated_images, rot_classes\n",
    "\n",
    "\n",
    "def eval_augmentations(config): \n",
    "    augment = config\n",
    "    print('called', augment)\n",
    "    augmentations = policy_decoder(augment, augment['num_policy'], augment['num_op'])\n",
    "    # Load the model from wandb. \n",
    "    fold = augment['cv_fold']\n",
    "    ckpt = args.checkpoint_fp + 'fold_%d.tar' %(fold)\n",
    "\n",
    "    model = load_model(cv_fold, args.loss).cuda()\n",
    "    model.eval()\n",
    "    loaders = []\n",
    "#     TODO: Undo this\n",
    "    for _ in range(args.num_policy): \n",
    "    # for _ in range(2):\n",
    "        _, validloader = get_dataloaders(augmentations, 128, kfold=fold)\n",
    "        loaders.append(iter(validloader))\n",
    "        del _\n",
    "\n",
    "    metrics = Accumulator()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    try: \n",
    "        with torch.no_grad(): \n",
    "            while True: \n",
    "                losses = []\n",
    "                corrects = []\n",
    "\n",
    "                for loader in loaders:\n",
    "                    data, label = next(loader)\n",
    "                    data = data.cuda()\n",
    "                    label = label.cuda()\n",
    "\n",
    "                    if args.loss == 'supervised':\n",
    "                        pred = model(data)\n",
    "\n",
    "                    if args.loss ==\"rotation\":\n",
    "                        rotated_images, label = rotate_images(data)\n",
    "                        pred = model(rotated_images)   \n",
    "                   \n",
    "\n",
    "                    loss = loss_fn(pred, label)\n",
    "                    losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                    _, pred = pred.topk(1, 1, True, True)\n",
    "                    pred = pred.t()\n",
    "                    correct = pred.eq(label.view(1, -1).expand_as(pred)).detach().cpu().numpy()\n",
    "                    corrects.append(correct)\n",
    "\n",
    "                    del loss, correct, pred, data, label\n",
    "\n",
    "                losses = np.concatenate(losses)\n",
    "                losses_min = np.min(losses, axis=0).squeeze()\n",
    "\n",
    "                corrects = np.concatenate(corrects)\n",
    "                corrects_max = np.max(corrects, axis=0).squeeze()\n",
    "                metrics.add_dict({ \n",
    "                    'minus_loss': -1*np.sum(losses_min),\n",
    "                    'correct': np.sum(corrects_max),\n",
    "                    'cnt': len(corrects_max)})\n",
    "                del corrects, corrects_max\n",
    "    \n",
    "    except StopIteration: \n",
    "        pass\n",
    "\n",
    "    del model\n",
    "    metrics = metrics/'cnt'\n",
    "    # reporter(minus_loss=metrics['minus_loss'], top_1_valid=metrics['correct'], done=True)\n",
    "    tune.track.log(top_1_valid=metrics['correct'])\n",
    "    print(metrics['correct'])\n",
    "    return metrics['correct']\n",
    "\n",
    "ops = augment_list(False) # Get the default augmentation set. \n",
    "# Define the space of our augmentations. \n",
    "space = {}\n",
    "for i in range(args.num_policy): \n",
    "    for j in range(args.num_op):\n",
    "        space['policy_%d_%d' %(i,j)]  = hp.choice('policy_%d_%d' %(i, j), list(range(0, len(ops))))\n",
    "        space['prob_%d_%d' %(i, j)] = hp.uniform('prob_%d_%d' %(i, j), 0.0, 1.0)\n",
    "        space['level_%d_%d' %(i, j)] = hp.uniform('level_%d_%d' %(i, j), 0.0, 1.0)\n",
    "\n",
    "final_policy_set = []\n",
    "reward_attr = 'top_1_valid' # TODO: let this be whatever we want. \n",
    "object_store_memory = int(0.6 * ray.utils.get_system_memory() // 10 ** 9 * 10 ** 9)\n",
    "ray.init(num_gpus=3, \n",
    "    num_cpus=28\n",
    "    )\n",
    "# ray.init(num_gpus=1, memory=200*1024*1024*100, object_store_memory=200*1024*1024*50)\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "cv_num = 5\n",
    "num_result_per_cv = 10\n",
    "\n",
    "for _ in range(2): \n",
    "    for cv_fold in range(cv_num): \n",
    "        name = \"slm_rotnet_search_%s_fold_%d\" %(args.dataid, cv_fold)\n",
    "        hyperopt_search=HyperOptSearch(space, \n",
    "            max_concurrent=3,\n",
    "            metric=reward_attr,\n",
    "            mode='max')\n",
    "\n",
    "\n",
    "        results = tune.run(\n",
    "            eval_augmentations, \n",
    "            name=name,\n",
    "            num_samples=200,\n",
    "            resources_per_trial={\n",
    "                \"gpu\": 1\n",
    "            },\n",
    "            search_alg=hyperopt_search,\n",
    "            verbose=2,\n",
    "            config = { \n",
    "                'num_op': args.num_op, \n",
    "                'num_policy': args.num_policy, \n",
    "                'cv_fold': cv_fold\n",
    "            },\n",
    "            return_trials=True,\n",
    "            stop={'training_iteration': 1},\n",
    "        )\n",
    "        results_copy = results\n",
    "        results = [x for x in results if x.last_result is not None]\n",
    "        results = sorted(results, key= lambda x: x.last_result[reward_attr], reverse=True)\n",
    "\n",
    "        for result in results[:num_result_per_cv]: \n",
    "            final_policy = policy_decoder(result.config, args.num_policy, args.num_op)\n",
    "            final_policy_set.extend(final_policy)\n",
    "\n",
    "        print(final_policy)\n",
    "print(final_policy_set)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(final_policy_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.environ)\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# =\"0,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_policy_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
