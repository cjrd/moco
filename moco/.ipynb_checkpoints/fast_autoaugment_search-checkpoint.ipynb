{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args <__main__.Args object at 0x7f28440aca20>\n",
      "<class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "/userdata/smetzger/all_deepul_files/ckpts/6bV5F_750epochs_512bsz_0.4000lr_mlp_cos_rrc_fold_0_0749.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 23:20:29,341\tINFO resource_spec.py:212 -- Starting Ray with 788.87 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-05 23:20:29,849\tINFO services.py:1148 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.6/1005.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/28 CPUs, 1/2 GPUs, 0.0/788.87 GiB heap, 0.0/128.52 GiB objects<br>Result logdir: /home/smetzger/ray_results/slm_moco_min_max_cifar10_fold_0<br>Number of trials: 2 (1 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  level_0_0</th><th style=\"text-align: right;\">  level_0_1</th><th style=\"text-align: right;\">  level_1_0</th><th style=\"text-align: right;\">  level_1_1</th><th style=\"text-align: right;\">  level_2_0</th><th style=\"text-align: right;\">  level_2_1</th><th style=\"text-align: right;\">  level_3_0</th><th style=\"text-align: right;\">  level_3_1</th><th style=\"text-align: right;\">  level_4_0</th><th style=\"text-align: right;\">  level_4_1</th><th style=\"text-align: right;\">  policy_0_0</th><th style=\"text-align: right;\">  policy_0_1</th><th style=\"text-align: right;\">  policy_1_0</th><th style=\"text-align: right;\">  policy_1_1</th><th style=\"text-align: right;\">  policy_2_0</th><th style=\"text-align: right;\">  policy_2_1</th><th style=\"text-align: right;\">  policy_3_0</th><th style=\"text-align: right;\">  policy_3_1</th><th style=\"text-align: right;\">  policy_4_0</th><th style=\"text-align: right;\">  policy_4_1</th><th style=\"text-align: right;\">  prob_0_0</th><th style=\"text-align: right;\">  prob_0_1</th><th style=\"text-align: right;\">  prob_1_0</th><th style=\"text-align: right;\">  prob_1_1</th><th style=\"text-align: right;\">  prob_2_0</th><th style=\"text-align: right;\">  prob_2_1</th><th style=\"text-align: right;\">  prob_3_0</th><th style=\"text-align: right;\">  prob_3_1</th><th style=\"text-align: right;\">  prob_4_0</th><th style=\"text-align: right;\">  prob_4_1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>eval_augmentations_af334c08</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">   0.533663</td><td style=\"text-align: right;\">  0.196065 </td><td style=\"text-align: right;\">   0.87612 </td><td style=\"text-align: right;\">   0.160602</td><td style=\"text-align: right;\">   0.538319</td><td style=\"text-align: right;\">   0.295442</td><td style=\"text-align: right;\">   0.261361</td><td style=\"text-align: right;\">   0.104141</td><td style=\"text-align: right;\">  0.788726 </td><td style=\"text-align: right;\">  0.826851 </td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.859749</td><td style=\"text-align: right;\">  0.702068</td><td style=\"text-align: right;\"> 0.0847497</td><td style=\"text-align: right;\">  0.783918</td><td style=\"text-align: right;\">  0.480362</td><td style=\"text-align: right;\"> 0.506242 </td><td style=\"text-align: right;\"> 0.0293253</td><td style=\"text-align: right;\">  0.774246</td><td style=\"text-align: right;\">   0.22296</td><td style=\"text-align: right;\">  0.904022</td></tr>\n",
       "<tr><td>eval_augmentations_af334c09</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">   0.961214</td><td style=\"text-align: right;\">  0.0528651</td><td style=\"text-align: right;\">   0.978601</td><td style=\"text-align: right;\">   0.511643</td><td style=\"text-align: right;\">   0.616741</td><td style=\"text-align: right;\">   0.13694 </td><td style=\"text-align: right;\">   0.571455</td><td style=\"text-align: right;\">   0.771929</td><td style=\"text-align: right;\">  0.0846368</td><td style=\"text-align: right;\">  0.0365349</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">  0.212404</td><td style=\"text-align: right;\">  0.694041</td><td style=\"text-align: right;\"> 0.554466 </td><td style=\"text-align: right;\">  0.351883</td><td style=\"text-align: right;\">  0.151513</td><td style=\"text-align: right;\"> 0.0778596</td><td style=\"text-align: right;\"> 0.46462  </td><td style=\"text-align: right;\">  0.406698</td><td style=\"text-align: right;\">   0.67867</td><td style=\"text-align: right;\">  0.732326</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m 2020-05-05 23:20:32,223\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m 2020-05-05 23:20:32,222\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m <class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m called {'num_op': 2, 'num_policy': 5, 'cv_fold': 0, 'level_0_0': 0.9612139698629089, 'level_0_1': 0.052865139811094974, 'level_1_0': 0.9786011663676307, 'level_1_1': 0.5116425034799855, 'level_2_0': 0.6167413743617322, 'level_2_1': 0.13694045602725213, 'level_3_0': 0.5714554842203294, 'level_3_1': 0.7719288109615726, 'level_4_0': 0.08463680781350469, 'level_4_1': 0.03653491193799796, 'policy_0_0': 6, 'policy_0_1': 6, 'policy_1_0': 0, 'policy_1_1': 12, 'policy_2_0': 11, 'policy_2_1': 4, 'policy_3_0': 11, 'policy_3_1': 12, 'policy_4_0': 2, 'policy_4_1': 4, 'prob_0_0': 0.21240397956898138, 'prob_0_1': 0.6940414748822257, 'prob_1_0': 0.5544661464173974, 'prob_1_1': 0.3518827582941513, 'prob_2_0': 0.15151281891344304, 'prob_2_1': 0.0778596014195927, 'prob_3_0': 0.4646198717829919, 'prob_3_1': 0.40669825928532133, 'prob_4_0': 0.6786701976487592, 'prob_4_1': 0.7323262549175965}\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m icl\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m <class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m called {'num_op': 2, 'num_policy': 5, 'cv_fold': 0, 'level_0_0': 0.5336632979081294, 'level_0_1': 0.196065482960337, 'level_1_0': 0.8761196578166047, 'level_1_1': 0.16060238780068825, 'level_2_0': 0.538319396077574, 'level_2_1': 0.2954420858411132, 'level_3_0': 0.2613607403909737, 'level_3_1': 0.10414113040581174, 'level_4_0': 0.7887258142923035, 'level_4_1': 0.8268512199036847, 'policy_0_0': 9, 'policy_0_1': 7, 'policy_1_0': 3, 'policy_1_1': 8, 'policy_2_0': 5, 'policy_2_1': 9, 'policy_3_0': 3, 'policy_3_1': 9, 'policy_4_0': 11, 'policy_4_1': 5, 'prob_0_0': 0.8597487627893224, 'prob_0_1': 0.7020680190270723, 'prob_1_0': 0.0847497389461851, 'prob_1_1': 0.783918390389995, 'prob_2_0': 0.4803619449620735, 'prob_2_1': 0.5062420674802607, 'prob_3_0': 0.029325292581372953, 'prob_3_1': 0.7742456043624418, 'prob_4_0': 0.22295965085326852, 'prob_4_1': 0.9040219766775376}\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m icl\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m /userdata/smetzger/all_deepul_files/ckpts/6bV5F_750epochs_512bsz_0.4000lr_mlp_cos_rrc_fold_0_0749.tar\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m /userdata/smetzger/all_deepul_files/ckpts/6bV5F_750epochs_512bsz_0.4000lr_mlp_cos_rrc_fold_0_0749.tar\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231097)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231092)\u001b[0m Files already downloaded and verified\n",
      "Result for eval_augmentations_af334c09:\n",
      "  date: 2020-05-05_23-21-13\n",
      "  done: false\n",
      "  experiment_id: 22e61dd9e70c4f35800d13248080a7f3\n",
      "  experiment_tag: 2_cv_fold=0,level_0_0=0.96121,level_0_1=0.052865,level_1_0=0.9786,level_1_1=0.51164,level_2_0=0.61674,level_2_1=0.13694,level_3_0=0.57146,level_3_1=0.77193,level_4_0=0.084637,level_4_1=0.036535,num_op=2,num_policy=5,policy_0_0=6,policy_0_1=6,policy_1_0=0,policy_1_1=12,policy_2_0=11,policy_2_1=4,policy_3_0=11,policy_3_1=12,policy_4_0=2,policy_4_1=4,prob_0_0=0.2124,prob_0_1=0.69404,prob_1_0=0.55447,prob_1_1=0.35188,prob_2_0=0.15151,prob_2_1=0.07786,prob_3_0=0.46462,prob_3_1=0.4067,prob_4_0=0.67867,prob_4_1=0.73233\n",
      "  hostname: spirit\n",
      "  iterations_since_restore: 1\n",
      "  minus_loss: 8.40012540588379\n",
      "  node_ip: 169.230.190.118\n",
      "  pid: 231097\n",
      "  plus_loss: -8.40012540588379\n",
      "  time_since_restore: 41.65046668052673\n",
      "  time_this_iter_s: 41.65046668052673\n",
      "  time_total_s: 41.65046668052673\n",
      "  timestamp: 1588746073\n",
      "  timesteps_since_restore: 0\n",
      "  top_1_valid: -0.4185\n",
      "  training_iteration: 0\n",
      "  trial_id: af334c09\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.6/1005.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/28 CPUs, 2/2 GPUs, 0.0/788.87 GiB heap, 0.0/128.52 GiB objects<br>Result logdir: /home/smetzger/ray_results/slm_moco_min_max_cifar10_fold_0<br>Number of trials: 2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  level_0_0</th><th style=\"text-align: right;\">  level_0_1</th><th style=\"text-align: right;\">  level_1_0</th><th style=\"text-align: right;\">  level_1_1</th><th style=\"text-align: right;\">  level_2_0</th><th style=\"text-align: right;\">  level_2_1</th><th style=\"text-align: right;\">  level_3_0</th><th style=\"text-align: right;\">  level_3_1</th><th style=\"text-align: right;\">  level_4_0</th><th style=\"text-align: right;\">  level_4_1</th><th style=\"text-align: right;\">  policy_0_0</th><th style=\"text-align: right;\">  policy_0_1</th><th style=\"text-align: right;\">  policy_1_0</th><th style=\"text-align: right;\">  policy_1_1</th><th style=\"text-align: right;\">  policy_2_0</th><th style=\"text-align: right;\">  policy_2_1</th><th style=\"text-align: right;\">  policy_3_0</th><th style=\"text-align: right;\">  policy_3_1</th><th style=\"text-align: right;\">  policy_4_0</th><th style=\"text-align: right;\">  policy_4_1</th><th style=\"text-align: right;\">  prob_0_0</th><th style=\"text-align: right;\">  prob_0_1</th><th style=\"text-align: right;\">  prob_1_0</th><th style=\"text-align: right;\">  prob_1_1</th><th style=\"text-align: right;\">  prob_2_0</th><th style=\"text-align: right;\">  prob_2_1</th><th style=\"text-align: right;\">  prob_3_0</th><th style=\"text-align: right;\">  prob_3_1</th><th style=\"text-align: right;\">  prob_4_0</th><th style=\"text-align: right;\">  prob_4_1</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>eval_augmentations_af334c08</td><td>RUNNING </td><td>                      </td><td style=\"text-align: right;\">   0.533663</td><td style=\"text-align: right;\">  0.196065 </td><td style=\"text-align: right;\">   0.87612 </td><td style=\"text-align: right;\">   0.160602</td><td style=\"text-align: right;\">   0.538319</td><td style=\"text-align: right;\">   0.295442</td><td style=\"text-align: right;\">   0.261361</td><td style=\"text-align: right;\">   0.104141</td><td style=\"text-align: right;\">  0.788726 </td><td style=\"text-align: right;\">  0.826851 </td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.859749</td><td style=\"text-align: right;\">  0.702068</td><td style=\"text-align: right;\"> 0.0847497</td><td style=\"text-align: right;\">  0.783918</td><td style=\"text-align: right;\">  0.480362</td><td style=\"text-align: right;\"> 0.506242 </td><td style=\"text-align: right;\"> 0.0293253</td><td style=\"text-align: right;\">  0.774246</td><td style=\"text-align: right;\">   0.22296</td><td style=\"text-align: right;\">  0.904022</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>eval_augmentations_af334c09</td><td>RUNNING </td><td>169.230.190.118:231097</td><td style=\"text-align: right;\">   0.961214</td><td style=\"text-align: right;\">  0.0528651</td><td style=\"text-align: right;\">   0.978601</td><td style=\"text-align: right;\">   0.511643</td><td style=\"text-align: right;\">   0.616741</td><td style=\"text-align: right;\">   0.13694 </td><td style=\"text-align: right;\">   0.571455</td><td style=\"text-align: right;\">   0.771929</td><td style=\"text-align: right;\">  0.0846368</td><td style=\"text-align: right;\">  0.0365349</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">  0.212404</td><td style=\"text-align: right;\">  0.694041</td><td style=\"text-align: right;\"> 0.554466 </td><td style=\"text-align: right;\">  0.351883</td><td style=\"text-align: right;\">  0.151513</td><td style=\"text-align: right;\"> 0.0778596</td><td style=\"text-align: right;\"> 0.46462  </td><td style=\"text-align: right;\">  0.406698</td><td style=\"text-align: right;\">   0.67867</td><td style=\"text-align: right;\">  0.732326</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">         41.6505</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for eval_augmentations_af334c08:\n",
      "  date: 2020-05-05_23-21-14\n",
      "  done: false\n",
      "  experiment_id: a103c5b3a25f4894838a631abaf1e1a6\n",
      "  experiment_tag: 1_cv_fold=0,level_0_0=0.53366,level_0_1=0.19607,level_1_0=0.87612,level_1_1=0.1606,level_2_0=0.53832,level_2_1=0.29544,level_3_0=0.26136,level_3_1=0.10414,level_4_0=0.78873,level_4_1=0.82685,num_op=2,num_policy=5,policy_0_0=9,policy_0_1=7,policy_1_0=3,policy_1_1=8,policy_2_0=5,policy_2_1=9,policy_3_0=3,policy_3_1=9,policy_4_0=11,policy_4_1=5,prob_0_0=0.85975,prob_0_1=0.70207,prob_1_0=0.08475,prob_1_1=0.78392,prob_2_0=0.48036,prob_2_1=0.50624,prob_3_0=0.029325,prob_3_1=0.77425,prob_4_0=0.22296,prob_4_1=0.90402\n",
      "  hostname: spirit\n",
      "  iterations_since_restore: 1\n",
      "  minus_loss: 8.871954151916503\n",
      "  node_ip: 169.230.190.118\n",
      "  pid: 231092\n",
      "  plus_loss: -8.871954151916503\n",
      "  time_since_restore: 42.133055686950684\n",
      "  time_this_iter_s: 42.133055686950684\n",
      "  time_total_s: 42.133055686950684\n",
      "  timestamp: 1588746074\n",
      "  timesteps_since_restore: 0\n",
      "  top_1_valid: -0.2886\n",
      "  training_iteration: 0\n",
      "  trial_id: af334c08\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m <class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m called {'num_op': 2, 'num_policy': 5, 'cv_fold': 0, 'level_0_0': 0.383815433745332, 'level_0_1': 0.3360770067093227, 'level_1_0': 0.5681589235345942, 'level_1_1': 0.009492935626575605, 'level_2_0': 0.6048704475116445, 'level_2_1': 0.8617144654648011, 'level_3_0': 0.23389893088555858, 'level_3_1': 0.9868161819368835, 'level_4_0': 0.7388372450406989, 'level_4_1': 0.21441025335131125, 'policy_0_0': 2, 'policy_0_1': 4, 'policy_1_0': 13, 'policy_1_1': 0, 'policy_2_0': 6, 'policy_2_1': 8, 'policy_3_0': 7, 'policy_3_1': 7, 'policy_4_0': 5, 'policy_4_1': 9, 'prob_0_0': 0.051611464274440966, 'prob_0_1': 0.16441922858136504, 'prob_1_0': 0.6753906743383334, 'prob_1_1': 0.7906196743452379, 'prob_2_0': 0.5057880034849139, 'prob_2_1': 0.5484429399296864, 'prob_3_0': 0.609670004318673, 'prob_3_1': 0.1948835880943538, 'prob_4_0': 0.4567435810015855, 'prob_4_1': 0.15955481155644657}\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m icl\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m 2020-05-05 23:21:15,423\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m 2020-05-05 23:21:15,853\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m <class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m called {'num_op': 2, 'num_policy': 5, 'cv_fold': 0, 'level_0_0': 0.9663803605114248, 'level_0_1': 0.8800224762693888, 'level_1_0': 0.26306516131653757, 'level_1_1': 0.5557173463975927, 'level_2_0': 0.7562125306149029, 'level_2_1': 0.07012029733469616, 'level_3_0': 0.8640489205650147, 'level_3_1': 0.3148768150514012, 'level_4_0': 0.7506847528678857, 'level_4_1': 0.3112748497519514, 'policy_0_0': 3, 'policy_0_1': 14, 'policy_1_0': 13, 'policy_1_1': 14, 'policy_2_0': 7, 'policy_2_1': 6, 'policy_3_0': 5, 'policy_3_1': 3, 'policy_4_0': 9, 'policy_4_1': 4, 'prob_0_0': 0.24665878541497166, 'prob_0_1': 0.36321253098830475, 'prob_1_0': 0.8901248730435133, 'prob_1_1': 0.3792900258646327, 'prob_2_0': 0.4861038002053718, 'prob_2_1': 0.15765214195751176, 'prob_3_0': 0.5385510530044708, 'prob_3_1': 0.44334127994108297, 'prob_4_0': 0.9230377353880941, 'prob_4_1': 0.5745162992897914}\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m icl\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m /userdata/smetzger/all_deepul_files/ckpts/6bV5F_750epochs_512bsz_0.4000lr_mlp_cos_rrc_fold_0_0749.tar\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m /userdata/smetzger/all_deepul_files/ckpts/6bV5F_750epochs_512bsz_0.4000lr_mlp_cos_rrc_fold_0_0749.tar\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231109)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=231176)\u001b[0m Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CODE is based of fastautoagument code here \n",
    "\n",
    "# https://github.com/kakaobrain/fast-autoaugment/blob/master/FastAutoAugment/search.py\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import ray \n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "from ray.tune import register_trainable, run_experiments\n",
    "import wandb\n",
    "import argparse\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/userdata/smetzger/all_deepul_files/deepul_proj/moco/\")\n",
    "import moco.loader\n",
    "import moco.builder\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "import os\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "\n",
    "# FOR DEBUG\n",
    "class Args:\n",
    "\n",
    "\n",
    "    base = 'moco_rrc'  # Moco or moco_rrc\n",
    "\n",
    "    # if base == 'moco': \n",
    "    #     checkpoints = ['fxrZE', 'lJu2W', 'rdEIg', 'esdq2' ,'vnhKs'] # Ordered KFOLDS order. Make this nicer.\n",
    "    \n",
    "    # else: \n",
    "    checkpoints = ['6bV5F', 'Pr3wZ', '43Hdo','h18r9', 'TcXI9']\n",
    "    checkpoint_fp = '/userdata/smetzger/all_deepul_files/ckpts'\n",
    "    data = '/userdata/smetzger/data/cifar_10/'\n",
    "    \n",
    "    # Some args for FAA. \n",
    "    num_op = 2\n",
    "    num_policy=5\n",
    "    num_search = 200\n",
    "    dataid = 'cifar10'\n",
    "    cv_ratio=1.0\n",
    "    smoke_test=False\n",
    "    resume=False\n",
    "    arch = 'resnet50'\n",
    "    distributed=False\n",
    "    loss = 'icl'# one of rotation, supervised, icl, icl_and_rotation.\n",
    "#     base = 'moco_rrc' # Name for what we are saving our training runs as.\n",
    "\n",
    "    # Moco args. \n",
    "    moco_k = 65536\n",
    "    moco_m = 0.999\n",
    "    moco_t = 0.2\n",
    "    \n",
    "    \n",
    "    # Whether or not to use the MLP for mocov2\n",
    "    mlp = True\n",
    "    \n",
    "    # Model input args for building the model head. \n",
    "    nomoco = False\n",
    "    rotnet = False\n",
    "    \n",
    "    moco_dim = 128\n",
    "    dim_mlp = 2048\n",
    "    \n",
    "    policy_dir = '/userdata/smetzger/all_deepul_files/policies'\n",
    "    \n",
    "    # Remember we are trying to max negative loss, so a negative here\n",
    "    # is like maximizing, a positive here is like minimizing. \n",
    "    loss_weights = {'rotation': 1, 'icl': -1, 'supervised':1} # weight for ICL, and ROTATION. Divide by mean.\n",
    "\n",
    "    \n",
    "args=Args()\n",
    "print('args', args)\n",
    "\n",
    "import random\n",
    "\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.transforms import Compose\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "random_mirror = True\n",
    "from self_aug.autoaug_scripts import augment_list, Augmentation, Accumulator\n",
    "\n",
    "# Define how we load our dataloaders. \n",
    "_CIFAR_MEAN, _CIFAR_STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "def get_dataloaders(augmentations, batch=1024, kfold=0, loss_type='icl', get_train=False):\n",
    "\n",
    "    \"\"\"\n",
    "    input: augmentations: the list of the augmentations you want applied to the data. \n",
    "    batch = batchsize, \n",
    "    kfold, which fold you want to look at (0, 1,2 3, or 4)\n",
    "    get_train, whether or not you want the train data. Use this when loading the data to train linear classifiers, \n",
    "    slash when you're loading the final classifier. \n",
    "    \"\"\"\n",
    "    if args.dataid == \"imagenet\":\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            traindir,\n",
    "            transformations)\n",
    "\n",
    "    elif args.dataid == \"cifar10\":\n",
    "\n",
    "        # THe default training transforms we use when training the CIFAR10 network. \n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(28, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "        \n",
    "\n",
    "        # If we're evaluating ICL, its only fair to do so with the ICL augmentations\n",
    "        if loss_type == \"icl\": \n",
    "            \n",
    "            random_resized_crop = transforms.RandomResizedCrop(28, scale=(0.2, 1.))\n",
    "           \n",
    "            \n",
    "            if 'rrc' in args.base: \n",
    "                print('using proper RRC')\n",
    "                transform_train = transforms.Compose([\n",
    "                    random_resized_crop, \n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD)\n",
    "                ])\n",
    "            \n",
    "            else: \n",
    "                transform_train = transforms.Compose([\n",
    "                random_resized_crop,\n",
    "                transforms.RandomApply([\n",
    "                    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "                ], p=0.8),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "                ])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Insert the new transforms in to the training transforms. \n",
    "        transform_train.transforms.insert(0, Augmentation(augmentations))\n",
    "        \n",
    "        # Use the twocrops transform. \n",
    "        if loss_type == \"icl\": \n",
    "            transform_train = moco.loader.TwoCropsTransform(transform_train)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Support for the following dataset is not yet implemented: {}\".format(args.dataid))\n",
    "\n",
    "    if get_train: \n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.data,\n",
    "                                                     transform=transform_train,\n",
    "                                                     download=True)\n",
    "\n",
    "    # In FAA They use Train Transform as well. \n",
    "    val_dataset = torchvision.datasets.CIFAR10(args.data, transform=transform_train, \n",
    "        download=True)\n",
    "    \n",
    "\n",
    "    if get_train: \n",
    "        torch.manual_seed(1337)\n",
    "        lengths = [len(train_dataset)//5]*5\n",
    "        folds = torch.utils.data.random_split(train_dataset, lengths)\n",
    "        folds.pop(kfold)\n",
    "        train_dataset = torch.utils.data.ConcatDataset(folds)\n",
    "\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    lengths = [len(val_dataset)//5]*5\n",
    "    folds = torch.utils.data.random_split(val_dataset, lengths)\n",
    "    val_dataset = folds[kfold]\n",
    "\n",
    "    if get_train: \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
    "            num_workers=8, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "    val_loader= torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, drop_last=False, \n",
    "        sampler=None\n",
    "    )\n",
    "\n",
    "    if not get_train: \n",
    "        train_loader = None\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "#  Go from the hyperopt parameters to the actual policy. \n",
    "def policy_decoder(augment, num_policy, num_op):\n",
    "    op_list = augment_list(False)\n",
    "    policies = []\n",
    "    for i in range(num_policy):\n",
    "        ops = []\n",
    "        for j in range(num_op):\n",
    "            op_idx = augment['policy_%d_%d' % (i, j)]\n",
    "            op_prob = augment['prob_%d_%d' % (i, j)]\n",
    "            op_level = augment['level_%d_%d' % (i, j)]\n",
    "            ops.append((op_list[op_idx][0].__name__, op_prob, op_level))\n",
    "        policies.append(ops)\n",
    "    return policies\n",
    "\n",
    "\n",
    "\n",
    "def find_model(name, fold, epochs=750, basepath=\"/userdata/smetzger/all_deepul_files/ckpts\"):\n",
    "    \"\"\"\n",
    "    name = model name\n",
    "    fold = which fold of the data to find. \n",
    "    epochs = how many epochs to load the checkpoint at (e.g. 750)\n",
    "    \"\"\"\n",
    "    for file in os.listdir(basepath):\n",
    "        if name in str(file) and 'fold_%d' %fold in str(file):\n",
    "            if str(file).endswith(str(epochs-1) + '.tar'): \n",
    "                return os.path.join(basepath, file)\n",
    "            \n",
    "    print(\"COULDNT FIND MODEL\")\n",
    "    \n",
    "\n",
    "def load_model(cv_fold, loss_type): \n",
    "    \n",
    "    model = models.__dict__[args.arch]()\n",
    "    \n",
    "    if args.dataid ==\"cifar10\":\n",
    "    # use the layer the SIMCLR authors used for cifar10 input conv, checked all padding/strides too.\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1,1), padding=(1,1), bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    if args.dataid == \"cifar10\":\n",
    "\n",
    "        if loss_type == 'supervised':\n",
    "            model.fc = torch.nn.Linear(model.fc.in_features, 10) # note this is for cifar 10.\n",
    "        elif loss_type =='rotation': \n",
    "            model.fc = torch.nn.Linear(model.fc.in_features, 4)\n",
    "\n",
    "    # Load the checkpoints. \n",
    "    if loss_type == 'supervised': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                \"{}_lincls_best.tar\".format(args.checkpoints[cv_fold]))\n",
    "    elif loss_type == 'rotation': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                 \"{}_lincls_best_rotation.tar\".format(args.checkpoints[cv_fold]))\n",
    "\n",
    "    \n",
    "    elif loss_type == 'icl': \n",
    "#         print('ICL')\n",
    "        heads = {}\n",
    "        if not args.nomoco:\n",
    "            heads[\"moco\"] = {\n",
    "            \"num_classes\": args.moco_dim\n",
    "        }\n",
    "        \n",
    "#         print(heads)\n",
    "\n",
    "        model = moco.builder.MoCo(\n",
    "            models.__dict__[args.arch],\n",
    "            K=args.moco_k, m=args.moco_m, T=args.moco_t, mlp=args.mlp, dataid=args.dataid,\n",
    "            multitask_heads=heads\n",
    "        )\n",
    "        savefile = find_model(args.checkpoints[cv_fold], cv_fold)\n",
    "        \n",
    "        print(savefile)\n",
    "        \n",
    "    ckpt = torch.load(savefile, map_location=\"cpu\")\n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith('module'):\n",
    "            state_dict[k[len(\"module.\"):]] = state_dict[k] \n",
    "            del state_dict[k]\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "m = load_model(0, args.loss)    \n",
    "\n",
    "def rotate_images(images):\n",
    "    nimages = images.shape[0]\n",
    "    n_rot_images = 4*nimages\n",
    "\n",
    "    # rotate images all 4 ways at once\n",
    "    rotated_images = torch.zeros([n_rot_images, images.shape[1], images.shape[2], images.shape[3]]).cuda()\n",
    "    rot_classes = torch.zeros([n_rot_images]).long().cuda()\n",
    "\n",
    "    rotated_images[:nimages] = images\n",
    "    # rotate 90\n",
    "    rotated_images[nimages:2*nimages] = images.flip(3).transpose(2,3)\n",
    "    rot_classes[nimages:2*nimages] = 1\n",
    "    # rotate 180\n",
    "    rotated_images[2*nimages:3*nimages] = images.flip(3).flip(2)\n",
    "    rot_classes[2*nimages:3*nimages] = 2\n",
    "    # rotate 270\n",
    "    rotated_images[3*nimages:4*nimages] = images.transpose(2,3).flip(3)\n",
    "    rot_classes[3*nimages:4*nimages] = 3\n",
    "\n",
    "    return rotated_images, rot_classes\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def eval_augmentations(config): \n",
    "    augment = config\n",
    "    print('called', augment)\n",
    "\n",
    "    # If we have ICL and rotation, loop over the two because of the different way we evaluate\n",
    "    # Either on top of the mocov2 augs, or on top of the lincls train augs. \n",
    "    if args.loss == 'icl_and_rotation':\n",
    "        losses = ['icl', 'rotation']\n",
    "    else: \n",
    "        losses = [args.loss]\n",
    "\n",
    "    metrics = Accumulator()\n",
    "    \n",
    "    for loss_type in losses: \n",
    "      \n",
    "        print(loss_type)\n",
    "        augmentations = policy_decoder(augment, augment['num_policy'], augment['num_op'])\n",
    "        \n",
    "        fold = augment['cv_fold']\n",
    "        print(cv_fold)\n",
    "\n",
    "        # Load the model, either the rotnet/lincls head, or the full MoCo model.\n",
    "        model = load_model(fold, loss_type).cuda()\n",
    "\n",
    "        model.eval()\n",
    "        loaders = []\n",
    "\n",
    "        for _ in range(args.num_policy): # there was a todo in the original moco code. basically we just load 5 loaders.\n",
    "            _, validloader = get_dataloaders(augmentations, 512, kfold=fold, loss_type=loss_type)\n",
    "            loaders.append(iter(validloader))\n",
    "            del _\n",
    "\n",
    "     \n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        try: \n",
    "            with torch.no_grad(): \n",
    "                while True: \n",
    "                    losses = []\n",
    "                    corrects = []\n",
    "\n",
    "                    for loader in loaders:\n",
    "\n",
    "                        if not loss_type == 'icl':\n",
    "\n",
    "                            data, label = next(loader)\n",
    "                            data = data.cuda()\n",
    "                            label = label.cuda()\n",
    "\n",
    "                            if loss_type == 'supervised':\n",
    "                                pred = model(data)\n",
    "\n",
    "                            if loss_type ==\"rotation\":\n",
    "                                rotated_images, label = rotate_images(data)\n",
    "                                pred = model(rotated_images)  \n",
    "\n",
    "                        else: \n",
    "\n",
    "                            images, _ = next(loader)\n",
    "                            images[0] = images[0].cuda(non_blocking=True)\n",
    "                            images[1] = images[1].cuda(non_blocking=True)\n",
    "                            pred, label =model(head=\"moco\", im_q=images[0], im_k=images[1], evaluate=True)\n",
    "                            acc = accuracy(pred, label)\n",
    "\n",
    "                        loss = loss_fn(pred, label)\n",
    "                        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                        _, pred = pred.topk(1, 1, True, True)\n",
    "                        pred = pred.t()\n",
    "                        correct = pred.eq(label.view(1, -1).expand_as(pred)).detach().cpu().numpy()\n",
    "                        corrects.append(correct)\n",
    "\n",
    "                        if not loss_type == 'icl':\n",
    "                            del loss, correct, pred, data, label\n",
    "                        else: \n",
    "                            del loss, images, pred, label, correct\n",
    "\n",
    "\n",
    "\n",
    "                    losses = np.concatenate(losses)\n",
    "#                     print(losses.shape)\n",
    "#                     print('losses shape' , losses.shape) This would usually just be 5*512, \n",
    "                    losses_min = np.mean(losses) # get it so it averages out intead of taking the smallest losses. \n",
    "                    corrects = np.concatenate(corrects)\n",
    "                    corrects_max = np.max(corrects, axis=0).squeeze() # 5, 512, \n",
    "#                     print('len corrects max', len(corrects_max), 'corrects shape', corrects.shape)\n",
    "                    losses_min *= losses.shape[0]/5 # Divide by the losses we're using. \n",
    "                    \n",
    "                    if loss_type == 'rotation': \n",
    "\n",
    "                        # Scale the loss by 1/4 since there are 4x as many samples. \n",
    "                        metrics.add_dict({ \n",
    "                            'minus_loss': -.25*np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'plus_loss': np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'correct': np.sum(corrects_max)*args.loss_weights[loss_type],\n",
    "                            'cnt': len(corrects_max)})\n",
    "                        del corrects, corrects_max\n",
    "                    else: \n",
    "                        metrics.add_dict({ \n",
    "                            'minus_loss': -1*np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'plus_loss': np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'correct': np.sum(corrects_max)*args.loss_weights[loss_type],\n",
    "                            'cnt': len(corrects_max)})\n",
    "                        del corrects, corrects_max\n",
    "        except StopIteration: \n",
    "            pass\n",
    "    \n",
    "    del model\n",
    "    metrics = metrics/'cnt'\n",
    "\n",
    "    # This is what it keeps track of for hyperopting. \n",
    "    tune.track.log(top_1_valid=metrics['correct'], minus_loss=metrics['minus_loss'], plus_loss=metrics['plus_loss'])\n",
    "    return metrics['minus_loss']\n",
    "\n",
    "ops = augment_list(False) # Get the default augmentation set. \n",
    "# Define the space of our augmentations. \n",
    "space = {}\n",
    "for i in range(args.num_policy): \n",
    "    for j in range(args.num_op):\n",
    "        space['policy_%d_%d' %(i,j)]  = hp.choice('policy_%d_%d' %(i, j), list(range(0, len(ops))))\n",
    "        space['prob_%d_%d' %(i, j)] = hp.uniform('prob_%d_%d' %(i, j), 0.0, 1.0)\n",
    "        space['level_%d_%d' %(i, j)] = hp.uniform('level_%d_%d' %(i, j), 0.0, 1.0)\n",
    "\n",
    "final_policy_set = []\n",
    "    \n",
    "reward_attr = 'minus_loss' \n",
    "\n",
    "# TODO boost this. \n",
    "ray.init(num_gpus=2, ignore_reinit_error=True, \n",
    "    num_cpus=28\n",
    ")\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "cv_num = 5\n",
    "num_result_per_cv = 10\n",
    "\n",
    "for _ in range(2):  # 2 experiments from FAA. \n",
    "    for cv_fold in range(cv_num): # For the 5 folds. \n",
    "        name = \"slm_moco_min_max_%s_fold_%d\" %(args.dataid, cv_fold)\n",
    "        hyperopt_search=HyperOptSearch(space, \n",
    "            max_concurrent=2, # TODO Up this. \n",
    "            metric=reward_attr,\n",
    "            mode='max')\n",
    "\n",
    "        results = tune.run(\n",
    "            eval_augmentations, \n",
    "            name=name,\n",
    "            num_samples=200,\n",
    "            resources_per_trial={\n",
    "                \"gpu\":1\n",
    "            },\n",
    "            search_alg=hyperopt_search,\n",
    "            verbose=2,\n",
    "            config = { \n",
    "                'num_op': args.num_op, \n",
    "                'num_policy': args.num_policy, \n",
    "                'cv_fold': cv_fold\n",
    "            },\n",
    "            return_trials=True,\n",
    "            stop={'training_iteration': 1},\n",
    "        )\n",
    "        results_copy = results\n",
    "        results = [x for x in results if x.last_result is not None]\n",
    "        results = sorted(results, key= lambda x: x.last_result[reward_attr], reverse=True)\n",
    "\n",
    "        for result in results[:num_result_per_cv]: \n",
    "            final_policy = policy_decoder(result.config, args.num_policy, args.num_op)\n",
    "            final_policy_set.extend(final_policy)\n",
    "            \n",
    "print(final_policy_set)\n",
    "\n",
    "# Start saving to a path called policies. \n",
    "import pickle\n",
    "savefp = os.path.join(args.policy_dir, str(args.base+'_'+args.loss+ '_' + (str(reward_attr) + '.pkl')))\n",
    "with open(savefp, 'wb') as f: \n",
    "    pickle.dump(final_policy_set, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
