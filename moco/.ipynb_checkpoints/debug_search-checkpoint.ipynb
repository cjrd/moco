{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args <__main__.Args object at 0x7fe65c43c4a8>\n",
      "<class 'self_aug.autoaug_scripts.Augmentation'>\n"
     ]
    }
   ],
   "source": [
    "# CODE is based of fastautoagument code here \n",
    "\n",
    "# https://github.com/kakaobrain/fast-autoaugment/blob/master/FastAutoAugment/search.py\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import ray \n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "from ray.tune import register_trainable, run_experiments\n",
    "import wandb\n",
    "import argparse\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/userdata/smetzger/all_deepul_files/deepul_proj/moco/\")\n",
    "import moco.loader\n",
    "import moco.builder\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "import os\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "\n",
    "# FOR DEBUG\n",
    "class Args:\n",
    "    checkpoints = ['fxrZE', 'lJu2W', 'rdEIg', 'esdq2' ,'vnhKs'] # Ordered KFOLDS order. Make this nicer.\n",
    "    checkpoint_fp = '/userdata/smetzger/all_deepul_files/ckpts'\n",
    "    data = '/userdata/smetzger/data/cifar_10/'\n",
    "    \n",
    "    # Some args for the Fast Autoaugment thing. \n",
    "    num_op = 2\n",
    "    num_policy=5\n",
    "    num_search = 200\n",
    "    dataid = 'cifar10'\n",
    "    cv_ratio=1.0\n",
    "    smoke_test=False\n",
    "    resume=False\n",
    "    arch = 'resnet50'\n",
    "    distributed=False\n",
    "    loss = 'icl'# one of rotation, supervised, icl, icl_and_rotation.\n",
    "    \n",
    "    # Moco args. \n",
    "    moco_k = 65536\n",
    "    moco_m = 0.999\n",
    "    moco_t = 0.2\n",
    "    \n",
    "    \n",
    "    # Whether or not to use the MLP for mocov2\n",
    "    mlp = True\n",
    "    \n",
    "    # Model input args for building the model head. \n",
    "    nomoco = False\n",
    "    rotnet = False\n",
    "    \n",
    "    moco_dim = 128\n",
    "    \n",
    "    \n",
    "args=Args()\n",
    "print('args', args)\n",
    "\n",
    "import random\n",
    "\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.transforms import Compose\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "random_mirror = True\n",
    "from self_aug.autoaug_scripts import augment_list, Augmentation, Accumulator\n",
    "\n",
    "# Define how we load our dataloaders. \n",
    "_CIFAR_MEAN, _CIFAR_STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(augmentations, batch=1024, kfold=0, get_train=False):\n",
    "\n",
    "    \"\"\"\n",
    "    input: augmentations: the list of the augmentations you want applied to the data. \n",
    "    batch = batchsize, \n",
    "    kfold, which fold you want to look at (0, 1,2 3, or 4)\n",
    "    get_train, whether or not you want the train data. Use this when loading the data to train linear classifiers, \n",
    "    slash when you're loading the final classifier. \n",
    "    \"\"\"\n",
    "    if args.dataid == \"imagenet\":\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            traindir,\n",
    "            transformations)\n",
    "\n",
    "        # TODO: add imagenet transforms etc. \n",
    "    elif args.dataid == \"cifar10\":\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(28, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "        \n",
    "        if args.loss == \"icl\": \n",
    "            \n",
    "            random_resized_crop = transforms.RandomResizedCrop(28, scale=(0.2, 1.))\n",
    "            \n",
    "            transform_train = transforms.Compose([\n",
    "            random_resized_crop,\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "            ])\n",
    "            \n",
    "  \n",
    "\n",
    "        transform_train.transforms.insert(0, Augmentation(augmentations))\n",
    "        \n",
    "        \n",
    "        if args.loss == \"icl\": \n",
    "            transform_train = moco.loader.TwoCropsTransform(transform_train)\n",
    "        \n",
    "        \n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.CenterCrop(28),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Support for the following dataset is not yet implemented: {}\".format(args.dataid))\n",
    "\n",
    "    if get_train: \n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.data,\n",
    "                                                     transform=transform_train,\n",
    "                                                     download=True)\n",
    "\n",
    "    # NOTE THAT IN THE FAA PAPER THE USED TRANSFORM TRAIN.  \n",
    "    \n",
    "    \n",
    "    val_dataset = torchvision.datasets.CIFAR10(args.data, transform=transform_train, \n",
    "        download=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    if get_train: \n",
    "        torch.manual_seed(1337)\n",
    "        lengths = [len(train_dataset)//5]*5\n",
    "        folds = torch.utils.data.random_split(train_dataset, lengths)\n",
    "        folds.pop(kfold)\n",
    "        train_dataset = torch.utils.data.ConcatDataset(folds)\n",
    "\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    lengths = [len(val_dataset)//5]*5\n",
    "    folds = torch.utils.data.random_split(val_dataset, lengths)\n",
    "    val_dataset = folds[kfold]\n",
    "\n",
    "    if get_train: \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
    "            num_workers=8, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "        \n",
    "    if args.loss == 'icl': \n",
    "        sampler =None\n",
    "        drop_last=False\n",
    "    else: \n",
    "        sampler =None\n",
    "        drop_last=False\n",
    "\n",
    "    val_loader= torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch, shuffle=True,\n",
    "        num_workers=8, pin_memory=True, drop_last=drop_last, \n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    if not get_train: \n",
    "        train_loader = None\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "tensor([ 0.0296, -0.0020,  0.0312,  ..., -0.0200, -0.0985,  0.0489])\n"
     ]
    }
   ],
   "source": [
    "# Take in the augment from hyperopt and return some augmentations, in teh way that we want them. \n",
    "def policy_decoder(augment, num_policy, num_op):\n",
    "    op_list = augment_list(False)\n",
    "    policies = []\n",
    "    for i in range(num_policy):\n",
    "        ops = []\n",
    "        for j in range(num_op):\n",
    "            op_idx = augment['policy_%d_%d' % (i, j)]\n",
    "            op_prob = augment['prob_%d_%d' % (i, j)]\n",
    "            op_level = augment['level_%d_%d' % (i, j)]\n",
    "            ops.append((op_list[op_idx][0].__name__, op_prob, op_level))\n",
    "        policies.append(ops)\n",
    "    return policies\n",
    "\n",
    "def find_model(name, fold, epochs=750, basepath=\"/userdata/smetzger/all_deepul_files/ckpts\"):\n",
    "    \"\"\"\n",
    "    name = model name\n",
    "    fold = which fold of the data to find. \n",
    "    epochs = how many epochs to load the checkpoint at (e.g. 750)\n",
    "    \n",
    "    \"\"\"\n",
    "    for file in os.listdir(basepath):\n",
    "        if name in str(file) and 'fold_%d' %fold in str(file):\n",
    "            if str(file).endswith(str(epochs-1) + '.tar'): \n",
    "                return os.path.join(basepath, file)\n",
    "            \n",
    "    print(\"COULDNT FIND MODEL\")\n",
    "    assert True==False # just throw an error. \n",
    "\n",
    "def load_model(cv_fold, loss_type): \n",
    "    \n",
    "    print(\"HELLO\")\n",
    "    model = models.__dict__[args.arch]()\n",
    "    # CIFAR 10 model\n",
    "    \n",
    "    if args.dataid ==\"cifar10\":\n",
    "    # use the layer the SIMCLR authors used for cifar10 input conv, checked all padding/strides too.\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1,1), padding=(1,1), bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    if args.dataid == \"cifar10\":\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 10) # note this is for cifar 10.\n",
    "\n",
    "\n",
    "    if loss_type == 'supervised': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                \"{}_lincls_best.tar\".format(args.checkpoints[cv_fold]))\n",
    "    elif loss_type == 'rotation': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                 \"{}_lincls_best_rotation.tar\".format(args.checkpoints[cv_fold]))\n",
    "\n",
    "    elif loss_type == 'icl': \n",
    "        \n",
    "#         print('ICL')\n",
    "        heads = {}\n",
    "        if not args.nomoco:\n",
    "            heads[\"moco\"] = {\n",
    "            \"num_classes\": args.moco_dim\n",
    "        }\n",
    "        \n",
    "#         print(heads)\n",
    "\n",
    "        model = moco.builder.MoCo(\n",
    "            models.__dict__[args.arch],\n",
    "            K=args.moco_k, m=args.moco_m, T=args.moco_t, mlp=args.mlp, dataid=args.dataid,\n",
    "            multitask_heads=heads\n",
    "        )\n",
    "        savefile = find_model(args.checkpoints[cv_fold], cv_fold)\n",
    "        \n",
    "#     print('savefile', savefile)\n",
    "    ckpt = torch.load(savefile, map_location=\"cpu\")\n",
    "    \n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    for k in list(state_dict.keys()):\n",
    "        # retain only encoder_q up to before the embedding layer\n",
    "        if k.startswith('module'):\n",
    "#             print(k)\n",
    "            # remove prefix\n",
    "            \n",
    "\n",
    "            state_dict[k[len(\"module.\"):]] = state_dict[k] \n",
    "            del state_dict[k]\n",
    "\n",
    "                \n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "m = load_model(0, args.loss)    \n",
    "\n",
    "print(m.queue[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_images(images):\n",
    "    nimages = images.shape[0]\n",
    "    n_rot_images = 4*nimages\n",
    "\n",
    "    # rotate images all 4 ways at once\n",
    "    rotated_images = torch.zeros([n_rot_images, images.shape[1], images.shape[2], images.shape[3]]).cuda()\n",
    "    rot_classes = torch.zeros([n_rot_images]).long().cuda()\n",
    "\n",
    "    rotated_images[:nimages] = images\n",
    "    # rotate 90\n",
    "    rotated_images[nimages:2*nimages] = images.flip(3).transpose(2,3)\n",
    "    rot_classes[nimages:2*nimages] = 1\n",
    "    # rotate 180\n",
    "    rotated_images[2*nimages:3*nimages] = images.flip(3).flip(2)\n",
    "    rot_classes[2*nimages:3*nimages] = 2\n",
    "    # rotate 270\n",
    "    rotated_images[3*nimages:4*nimages] = images.transpose(2,3).flip(3)\n",
    "    rot_classes[3*nimages:4*nimages] = 3\n",
    "\n",
    "    return rotated_images, rot_classes\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def eval_augmentations(config): \n",
    "    augment = config\n",
    "    print('called', augment)\n",
    "    augmentations = policy_decoder(augment, augment['num_policy'], augment['num_op'])\n",
    "    # Load the model from wandb. \n",
    "    fold = augment['cv_fold']\n",
    "    ckpt = args.checkpoint_fp + 'fold_%d.tar' %(fold)\n",
    "\n",
    "    model = load_model(cv_fold, args.loss).cuda()\n",
    "    model.eval()\n",
    "    loaders = []\n",
    "    \n",
    "    for _ in range(args.num_policy): #TODO: \n",
    "        _, validloader = get_dataloaders(augmentations, 512, kfold=fold)\n",
    "        loaders.append(iter(validloader))\n",
    "        del _\n",
    "\n",
    "           \n",
    "        \n",
    "    metrics = Accumulator()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "\n",
    "    try: \n",
    "        \n",
    "        i = 0\n",
    "        with torch.no_grad(): \n",
    "            while True: \n",
    "                losses = []\n",
    "                corrects = []\n",
    "\n",
    "                for loader in loaders:\n",
    "                    \n",
    "                    if not args.loss == 'icl':\n",
    "                        \n",
    "                        \n",
    "                        data, label = next(loader)\n",
    "                        data = data.cuda()\n",
    "                        label = label.cuda()\n",
    "\n",
    "                        if args.loss == 'supervised':\n",
    "                            pred = model(data)\n",
    "\n",
    "                        if args.loss ==\"rotation\":\n",
    "                            rotated_images, label = rotate_images(data)\n",
    "                            pred = model(rotated_images)  \n",
    "                            \n",
    "                    else: \n",
    "                        \n",
    "                        images, _ = next(loader)\n",
    "                        images[0] = images[0].cuda(non_blocking=True)\n",
    "                        images[1] = images[1].cuda(non_blocking=True)\n",
    "#                         print(images[0] == images[1])\n",
    "                        pred, label =model(head=\"moco\", im_q=images[0], im_k=images[1], evaluate=True)\n",
    "                        \n",
    "                        acc = accuracy(pred, label)\n",
    "            \n",
    "#                         print(acc)\n",
    "\n",
    "\n",
    "                    loss = loss_fn(pred, label)\n",
    "                    losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                    _, pred = pred.topk(1, 1, True, True)\n",
    "                    pred = pred.t()\n",
    "                    correct = pred.eq(label.view(1, -1).expand_as(pred)).detach().cpu().numpy()\n",
    "                    corrects.append(correct)\n",
    "                    \n",
    "                    if not args.loss == 'icl':\n",
    "                        del loss, correct, pred, data, label\n",
    "                    else: \n",
    "                        del loss, images, pred, label, correct\n",
    "    \n",
    "    \n",
    "    \n",
    "                losses = np.concatenate(losses)\n",
    "#                 print(losses.shape)\n",
    "#                 print('mean loss', np.mean(losses))\n",
    "                \n",
    "                \n",
    "                \n",
    "                #losses_min = np.min(losses, axis=0).squeeze()\n",
    "                \n",
    "                losses_min = np.mean(losses) # get it so it averages out.\n",
    "                corrects = np.concatenate(corrects)\n",
    "#                 print(corrects.shape)\n",
    "#                 print('corrects[0]', corrects[0])\n",
    "                corrects_max = np.max(corrects, axis=0).squeeze()\n",
    "                losses_min *= len(corrects_max)\n",
    "                metrics.add_dict({ \n",
    "                    'minus_loss': -1*np.sum(losses_min),\n",
    "                    'correct': np.sum(corrects_max),\n",
    "                    'cnt': len(corrects_max)})\n",
    "        \n",
    "#                 print(metrics['minus_loss'])\n",
    "                del corrects, corrects_max\n",
    "\n",
    "    \n",
    "    except StopIteration: \n",
    "        pass\n",
    "\n",
    "    del model\n",
    "    metrics = metrics/'cnt'\n",
    "    # reporter(minus_loss=metrics['minus_loss'], top_1_valid=metrics['correct'], done=True)\n",
    "    tune.track.log(top_1_valid=metrics['correct'], minus_loss=metrics['minus_loss'])\n",
    "    print(metrics['correct'])\n",
    "    return metrics['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 18:03:38,285\tINFO resource_spec.py:212 -- Starting Ray with 772.07 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-01 18:03:38,765\tINFO services.py:1148 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.4/1005.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/28 CPUs, 1/3 GPUs, 0.0/772.07 GiB heap, 0.0/128.52 GiB objects<br>Result logdir: /home/smetzger/ray_results/slm_rotnet_search_cifar10_fold_2<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  level_0_0</th><th style=\"text-align: right;\">  level_0_1</th><th style=\"text-align: right;\">  level_1_0</th><th style=\"text-align: right;\">  level_1_1</th><th style=\"text-align: right;\">  level_2_0</th><th style=\"text-align: right;\">  level_2_1</th><th style=\"text-align: right;\">  level_3_0</th><th style=\"text-align: right;\">  level_3_1</th><th style=\"text-align: right;\">  level_4_0</th><th style=\"text-align: right;\">  level_4_1</th><th style=\"text-align: right;\">  policy_0_0</th><th style=\"text-align: right;\">  policy_0_1</th><th style=\"text-align: right;\">  policy_1_0</th><th style=\"text-align: right;\">  policy_1_1</th><th style=\"text-align: right;\">  policy_2_0</th><th style=\"text-align: right;\">  policy_2_1</th><th style=\"text-align: right;\">  policy_3_0</th><th style=\"text-align: right;\">  policy_3_1</th><th style=\"text-align: right;\">  policy_4_0</th><th style=\"text-align: right;\">  policy_4_1</th><th style=\"text-align: right;\">  prob_0_0</th><th style=\"text-align: right;\">  prob_0_1</th><th style=\"text-align: right;\">  prob_1_0</th><th style=\"text-align: right;\">  prob_1_1</th><th style=\"text-align: right;\">  prob_2_0</th><th style=\"text-align: right;\">  prob_2_1</th><th style=\"text-align: right;\">  prob_3_0</th><th style=\"text-align: right;\">  prob_3_1</th><th style=\"text-align: right;\">  prob_4_0</th><th style=\"text-align: right;\">  prob_4_1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>eval_augmentations_c1fc4230</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">   0.915404</td><td style=\"text-align: right;\">   0.857497</td><td style=\"text-align: right;\">   0.332748</td><td style=\"text-align: right;\">   0.815383</td><td style=\"text-align: right;\">   0.414586</td><td style=\"text-align: right;\">   0.296094</td><td style=\"text-align: right;\">   0.906013</td><td style=\"text-align: right;\">   0.895476</td><td style=\"text-align: right;\">   0.735576</td><td style=\"text-align: right;\">   0.113118</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">          14</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">  0.171136</td><td style=\"text-align: right;\">  0.428876</td><td style=\"text-align: right;\"> 0.0868064</td><td style=\"text-align: right;\">   0.77022</td><td style=\"text-align: right;\">  0.123359</td><td style=\"text-align: right;\">  0.793753</td><td style=\"text-align: right;\">  0.766988</td><td style=\"text-align: right;\">  0.382436</td><td style=\"text-align: right;\">  0.520684</td><td style=\"text-align: right;\">  0.229371</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m 2020-05-01 18:03:41,059\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m called {'num_op': 2, 'num_policy': 5, 'cv_fold': 2, 'level_0_0': 0.9154042825481034, 'level_0_1': 0.8574973734254269, 'level_1_0': 0.3327480048299556, 'level_1_1': 0.8153831780143229, 'level_2_0': 0.4145858737679482, 'level_2_1': 0.2960935738817615, 'level_3_0': 0.9060126708681504, 'level_3_1': 0.8954760265775837, 'level_4_0': 0.7355757259623835, 'level_4_1': 0.11311838738784763, 'policy_0_0': 5, 'policy_0_1': 3, 'policy_1_0': 12, 'policy_1_1': 3, 'policy_2_0': 14, 'policy_2_1': 4, 'policy_3_0': 2, 'policy_3_1': 3, 'policy_4_0': 0, 'policy_4_1': 10, 'prob_0_0': 0.17113553729087583, 'prob_0_1': 0.42887648776159937, 'prob_1_0': 0.08680636320483914, 'prob_1_1': 0.77022044353077, 'prob_2_0': 0.12335912320729903, 'prob_2_1': 0.7937526656065451, 'prob_3_0': 0.7669882608574401, 'prob_3_1': 0.3824363255106594, 'prob_4_0': 0.5206840134433035, 'prob_4_1': 0.22937053804810537}\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m HELLO\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f62676fc048>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f62676fc048>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335fc3dba8>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335fc3dba8>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335f3a3978>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335f3a3978>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335f384b70>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335f384b70>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335e4d86d8>\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f335e4d86d8>\n",
      "Result for eval_augmentations_c1fc4230:\n",
      "  date: 2020-05-01_18-04-23\n",
      "  done: false\n",
      "  experiment_id: 745db6e7f487476a81272df9c7bdc076\n",
      "  experiment_tag: 1_cv_fold=2,level_0_0=0.9154,level_0_1=0.8575,level_1_0=0.33275,level_1_1=0.81538,level_2_0=0.41459,level_2_1=0.29609,level_3_0=0.90601,level_3_1=0.89548,level_4_0=0.73558,level_4_1=0.11312,num_op=2,num_policy=5,policy_0_0=5,policy_0_1=3,policy_1_0=12,policy_1_1=3,policy_2_0=14,policy_2_1=4,policy_3_0=2,policy_3_1=3,policy_4_0=0,policy_4_1=10,prob_0_0=0.17114,prob_0_1=0.42888,prob_1_0=0.086806,prob_1_1=0.77022,prob_2_0=0.12336,prob_2_1=0.79375,prob_3_0=0.76699,prob_3_1=0.38244,prob_4_0=0.52068,prob_4_1=0.22937\n",
      "  hostname: spirit\n",
      "  iterations_since_restore: 1\n",
      "  minus_loss: -8.182392706298828\n",
      "  node_ip: 169.230.190.118\n",
      "  pid: 473172\n",
      "  time_since_restore: 42.39925742149353\n",
      "  time_this_iter_s: 42.39925742149353\n",
      "  time_total_s: 42.39925742149353\n",
      "  timestamp: 1588381463\n",
      "  timesteps_since_restore: 0\n",
      "  top_1_valid: 0.2138\n",
      "  training_iteration: 0\n",
      "  trial_id: c1fc4230\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=473172)\u001b[0m 0.2138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 62.0/1005.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/28 CPUs, 1/3 GPUs, 0.0/772.07 GiB heap, 0.0/128.52 GiB objects<br>Result logdir: /home/smetzger/ray_results/slm_rotnet_search_cifar10_fold_2<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  level_0_0</th><th style=\"text-align: right;\">  level_0_1</th><th style=\"text-align: right;\">  level_1_0</th><th style=\"text-align: right;\">  level_1_1</th><th style=\"text-align: right;\">  level_2_0</th><th style=\"text-align: right;\">  level_2_1</th><th style=\"text-align: right;\">  level_3_0</th><th style=\"text-align: right;\">  level_3_1</th><th style=\"text-align: right;\">  level_4_0</th><th style=\"text-align: right;\">  level_4_1</th><th style=\"text-align: right;\">  policy_0_0</th><th style=\"text-align: right;\">  policy_0_1</th><th style=\"text-align: right;\">  policy_1_0</th><th style=\"text-align: right;\">  policy_1_1</th><th style=\"text-align: right;\">  policy_2_0</th><th style=\"text-align: right;\">  policy_2_1</th><th style=\"text-align: right;\">  policy_3_0</th><th style=\"text-align: right;\">  policy_3_1</th><th style=\"text-align: right;\">  policy_4_0</th><th style=\"text-align: right;\">  policy_4_1</th><th style=\"text-align: right;\">  prob_0_0</th><th style=\"text-align: right;\">  prob_0_1</th><th style=\"text-align: right;\">  prob_1_0</th><th style=\"text-align: right;\">  prob_1_1</th><th style=\"text-align: right;\">  prob_2_0</th><th style=\"text-align: right;\">  prob_2_1</th><th style=\"text-align: right;\">  prob_3_0</th><th style=\"text-align: right;\">  prob_3_1</th><th style=\"text-align: right;\">  prob_4_0</th><th style=\"text-align: right;\">  prob_4_1</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>eval_augmentations_c1fc4230</td><td>RUNNING </td><td>169.230.190.118:473172</td><td style=\"text-align: right;\">   0.915404</td><td style=\"text-align: right;\">   0.857497</td><td style=\"text-align: right;\">   0.332748</td><td style=\"text-align: right;\">   0.815383</td><td style=\"text-align: right;\">   0.414586</td><td style=\"text-align: right;\">   0.296094</td><td style=\"text-align: right;\">   0.906013</td><td style=\"text-align: right;\">   0.895476</td><td style=\"text-align: right;\">   0.735576</td><td style=\"text-align: right;\">   0.113118</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">          14</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">  0.171136</td><td style=\"text-align: right;\">  0.428876</td><td style=\"text-align: right;\"> 0.0868064</td><td style=\"text-align: right;\">   0.77022</td><td style=\"text-align: right;\">  0.123359</td><td style=\"text-align: right;\">  0.793753</td><td style=\"text-align: right;\">  0.766988</td><td style=\"text-align: right;\">  0.382436</td><td style=\"text-align: right;\">  0.520684</td><td style=\"text-align: right;\">  0.229371</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">         42.3993</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m 2020-05-01 18:04:25,357\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <class 'self_aug.autoaug_scripts.Augmentation'>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m called {'num_op': 2, 'num_policy': 5, 'cv_fold': 2, 'level_0_0': 0.427199199614379, 'level_0_1': 0.06359013957407589, 'level_1_0': 0.9952128058198416, 'level_1_1': 0.5113940193197524, 'level_2_0': 0.6091201894720152, 'level_2_1': 0.3866621188619168, 'level_3_0': 0.1903880855883907, 'level_3_1': 0.3500567814432102, 'level_4_0': 0.36586860583664826, 'level_4_1': 0.8610398389894555, 'policy_0_0': 3, 'policy_0_1': 0, 'policy_1_0': 11, 'policy_1_1': 11, 'policy_2_0': 8, 'policy_2_1': 11, 'policy_3_0': 3, 'policy_3_1': 4, 'policy_4_0': 4, 'policy_4_1': 7, 'prob_0_0': 0.8727950350356317, 'prob_0_1': 0.17070093914707374, 'prob_1_0': 0.18271100928680029, 'prob_1_1': 0.9480649613841009, 'prob_2_0': 0.5937987502028733, 'prob_2_1': 0.8175820169832232, 'prob_3_0': 0.5705874189009587, 'prob_3_1': 0.006496307498675469, 'prob_4_0': 0.6506818174087393, 'prob_4_1': 0.17106292001718248}\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m HELLO\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f4ed13f7cf8>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f4ed13f7cf8>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc98d4ba8>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc98d4ba8>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc9034c18>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc9034c18>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc9015e80>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc9015e80>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc8211780>\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=473168)\u001b[0m <moco.loader.TwoCropsTransform object at 0x7f1fc8211780>\n"
     ]
    }
   ],
   "source": [
    "ops = augment_list(False) # Get the default augmentation set. \n",
    "# Define the space of our augmentations. \n",
    "space = {}\n",
    "for i in range(args.num_policy): \n",
    "    for j in range(args.num_op):\n",
    "        space['policy_%d_%d' %(i,j)]  = hp.choice('policy_%d_%d' %(i, j), list(range(0, len(ops))))\n",
    "        space['prob_%d_%d' %(i, j)] = hp.uniform('prob_%d_%d' %(i, j), 0.0, 1.0)\n",
    "        space['level_%d_%d' %(i, j)] = hp.uniform('level_%d_%d' %(i, j), 0.0, 1.0)\n",
    "\n",
    "final_policy_set = []\n",
    "\n",
    "if not args.loss == 'icl': \n",
    "    reward_attr = 'top_1_valid'\n",
    "else: \n",
    "    reward_attr = 'minus_loss'\n",
    "    \n",
    "    \n",
    "# TODO: let this be whatever we want. \n",
    "object_store_memory = int(0.6 * ray.utils.get_system_memory() // 10 ** 9 * 10 ** 9)\n",
    "ray.init(num_gpus=3, ignore_reinit_error=True, \n",
    "    num_cpus=28\n",
    "    )\n",
    "# ray.init(num_gpus=1, memory=200*1024*1024*100, object_store_memory=200*1024*1024*50)\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "cv_num = 5\n",
    "num_result_per_cv = 10\n",
    "\n",
    "for _ in range(2): \n",
    "    for cv_fold in range(2, cv_num): \n",
    "        name = \"slm_rotnet_search_%s_fold_%d\" %(args.dataid, cv_fold)\n",
    "        hyperopt_search=HyperOptSearch(space, \n",
    "            max_concurrent=1,\n",
    "            metric=reward_attr,\n",
    "            mode='max')\n",
    "\n",
    "\n",
    "        results = tune.run(\n",
    "            eval_augmentations, \n",
    "            name=name,\n",
    "            num_samples=200,\n",
    "            resources_per_trial={\n",
    "                \"gpu\": 1\n",
    "            },\n",
    "            search_alg=hyperopt_search,\n",
    "            verbose=2,\n",
    "            config = { \n",
    "                'num_op': args.num_op, \n",
    "                'num_policy': args.num_policy, \n",
    "                'cv_fold': cv_fold\n",
    "            },\n",
    "            return_trials=True,\n",
    "            stop={'training_iteration': 1},\n",
    "        )\n",
    "        results_copy = results\n",
    "        results = [x for x in results if x.last_result is not None]\n",
    "        results = sorted(results, key= lambda x: x.last_result[reward_attr], reverse=True)\n",
    "\n",
    "        for result in results[:num_result_per_cv]: \n",
    "            final_policy = policy_decoder(result.config, args.num_policy, args.num_op)\n",
    "            final_policy_set.extend(final_policy)\n",
    "\n",
    "        print(final_policy)\n",
    "print(final_policy_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
