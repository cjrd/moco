{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE is based of fastautoagument code here \n",
    "\n",
    "# https://github.com/kakaobrain/fast-autoaugment/blob/master/FastAutoAugment/search.py\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import ray \n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "from ray.tune import register_trainable, run_experiments\n",
    "import wandb\n",
    "import argparse\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/userdata/smetzger/all_deepul_files/deepul_proj/moco/\")\n",
    "import moco.loader\n",
    "import moco.builder\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "import os\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "\n",
    "# FOR DEBUG\n",
    "class Args:\n",
    "    checkpoints = ['fxrZE', 'lJu2W', 'rdEIg', 'esdq2' ,'vnhKs'] # Ordered KFOLDS order. Make this nicer.\n",
    "    checkpoint_fp = '/userdata/smetzger/all_deepul_files/ckpts'\n",
    "    data = '/userdata/smetzger/data/cifar_10/'\n",
    "    \n",
    "    # Some args for the Fast Autoaugment thing. \n",
    "    num_op = 2\n",
    "    num_policy=5\n",
    "    num_search = 200\n",
    "    dataid = 'cifar10'\n",
    "    cv_ratio=1.0\n",
    "    smoke_test=False\n",
    "    resume=False\n",
    "    arch = 'resnet50'\n",
    "    distributed=False\n",
    "    loss = 'icl_and_rotation'# one of rotation, supervised, icl, icl_and_rotation.\n",
    "    base = 'moco' # Name for what we are saving our training runs as.\n",
    "\n",
    "    # Moco args. \n",
    "    moco_k = 65536\n",
    "    moco_m = 0.999\n",
    "    moco_t = 0.2\n",
    "    \n",
    "    \n",
    "    # Whether or not to use the MLP for mocov2\n",
    "    mlp = True\n",
    "    \n",
    "    # Model input args for building the model head. \n",
    "    nomoco = False\n",
    "    rotnet = False\n",
    "    \n",
    "    moco_dim = 128\n",
    "    policy_dir = '/userdata/smetzger/all_deepul_files/policies'\n",
    "    \n",
    "    # Remember we are trying to max negative loss, so a negative here\n",
    "    # is like maximizing, a positive here is like minimizing. \n",
    "    loss_weights = {'rotation': 1, 'icl': -1, 'supervised':1} # weight for ICL, and ROTATION.\n",
    "\n",
    "    \n",
    "args=Args()\n",
    "print('args', args)\n",
    "\n",
    "import random\n",
    "\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.transforms import Compose\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "random_mirror = True\n",
    "from self_aug.autoaug_scripts import augment_list, Augmentation, Accumulator\n",
    "\n",
    "# Define how we load our dataloaders. \n",
    "_CIFAR_MEAN, _CIFAR_STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "def get_dataloaders(augmentations, batch=1024, kfold=0, loss_type='icl', get_train=False):\n",
    "\n",
    "    \"\"\"\n",
    "    input: augmentations: the list of the augmentations you want applied to the data. \n",
    "    batch = batchsize, \n",
    "    kfold, which fold you want to look at (0, 1,2 3, or 4)\n",
    "    get_train, whether or not you want the train data. Use this when loading the data to train linear classifiers, \n",
    "    slash when you're loading the final classifier. \n",
    "    \"\"\"\n",
    "    if args.dataid == \"imagenet\":\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            traindir,\n",
    "            transformations)\n",
    "\n",
    "        # TODO: add imagenet transforms etc. \n",
    "    elif args.dataid == \"cifar10\":\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(28, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "        \n",
    "        if loss_type == \"icl\": \n",
    "            \n",
    "            random_resized_crop = transforms.RandomResizedCrop(28, scale=(0.2, 1.))\n",
    "            \n",
    "            transform_train = transforms.Compose([\n",
    "            random_resized_crop,\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "            ])\n",
    "            \n",
    "        transform_train.transforms.insert(0, Augmentation(augmentations))\n",
    "        \n",
    "        \n",
    "        if loss_type == \"icl\": \n",
    "            transform_train = moco.loader.TwoCropsTransform(transform_train)\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.CenterCrop(28),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(_CIFAR_MEAN, _CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Support for the following dataset is not yet implemented: {}\".format(args.dataid))\n",
    "\n",
    "    if get_train: \n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.data,\n",
    "                                                     transform=transform_train,\n",
    "                                                     download=True)\n",
    "\n",
    "    # NOTE THAT IN THE FAA PAPER THE USED TRANSFORM TRAIN.  \n",
    "    \n",
    "    \n",
    "    val_dataset = torchvision.datasets.CIFAR10(args.data, transform=transform_train, \n",
    "        download=True)\n",
    "    \n",
    "\n",
    "    if get_train: \n",
    "        torch.manual_seed(1337)\n",
    "        lengths = [len(train_dataset)//5]*5\n",
    "        folds = torch.utils.data.random_split(train_dataset, lengths)\n",
    "        folds.pop(kfold)\n",
    "        train_dataset = torch.utils.data.ConcatDataset(folds)\n",
    "\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    lengths = [len(val_dataset)//5]*5\n",
    "    folds = torch.utils.data.random_split(val_dataset, lengths)\n",
    "    val_dataset = folds[kfold]\n",
    "\n",
    "    if get_train: \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
    "            num_workers=8, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "\n",
    "        \n",
    "    if loss_type == 'icl': \n",
    "        sampler =None\n",
    "        drop_last=False\n",
    "    else: \n",
    "        sampler =None\n",
    "        drop_last=False\n",
    "\n",
    "    val_loader= torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, drop_last=drop_last, \n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    if not get_train: \n",
    "        train_loader = None\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Take in the augment from hyperopt and return some augmentations, in teh way that we want them. \n",
    "def policy_decoder(augment, num_policy, num_op):\n",
    "    op_list = augment_list(False)\n",
    "    policies = []\n",
    "    for i in range(num_policy):\n",
    "        ops = []\n",
    "        for j in range(num_op):\n",
    "            op_idx = augment['policy_%d_%d' % (i, j)]\n",
    "            op_prob = augment['prob_%d_%d' % (i, j)]\n",
    "            op_level = augment['level_%d_%d' % (i, j)]\n",
    "            ops.append((op_list[op_idx][0].__name__, op_prob, op_level))\n",
    "        policies.append(ops)\n",
    "    return policies\n",
    "\n",
    "def find_model(name, fold, epochs=750, basepath=\"/userdata/smetzger/all_deepul_files/ckpts\"):\n",
    "    \"\"\"\n",
    "    name = model name\n",
    "    fold = which fold of the data to find. \n",
    "    epochs = how many epochs to load the checkpoint at (e.g. 750)\n",
    "    \n",
    "    \"\"\"\n",
    "    for file in os.listdir(basepath):\n",
    "        if name in str(file) and 'fold_%d' %fold in str(file):\n",
    "            if str(file).endswith(str(epochs-1) + '.tar'): \n",
    "                return os.path.join(basepath, file)\n",
    "            \n",
    "    print(\"COULDNT FIND MODEL\")\n",
    "    assert True==False # just throw an error. \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def load_model(cv_fold, loss_type): \n",
    "    \n",
    "    print(\"HELLO\")\n",
    "    model = models.__dict__[args.arch]()\n",
    "    # CIFAR 10 model\n",
    "    \n",
    "    if args.dataid ==\"cifar10\":\n",
    "    # use the layer the SIMCLR authors used for cifar10 input conv, checked all padding/strides too.\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1,1), padding=(1,1), bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    if args.dataid == \"cifar10\":\n",
    "\n",
    "        if loss_type == 'supervised':\n",
    "            model.fc = torch.nn.Linear(model.fc.in_features, 10) # note this is for cifar 10.\n",
    "        elif loss_type =='rotation': \n",
    "            model.fc = torch.nn.Linear(model.fc.in_features, 4)\n",
    "\n",
    "\n",
    "    if loss_type == 'supervised': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                \"{}_lincls_best.tar\".format(args.checkpoints[cv_fold]))\n",
    "    elif loss_type == 'rotation': \n",
    "        savefile = os.path.join(args.checkpoint_fp, \n",
    "                                 \"{}_lincls_best_rotation.tar\".format(args.checkpoints[cv_fold]))\n",
    "\n",
    "    elif loss_type == 'icl' or loss_type == 'icl_and_rotation': \n",
    "#         print('ICL')\n",
    "        heads = {}\n",
    "        if not args.nomoco:\n",
    "            heads[\"moco\"] = {\n",
    "            \"num_classes\": args.moco_dim\n",
    "        }\n",
    "        \n",
    "#         print(heads)\n",
    "\n",
    "        model = moco.builder.MoCo(\n",
    "            models.__dict__[args.arch],\n",
    "            K=args.moco_k, m=args.moco_m, T=args.moco_t, mlp=args.mlp, dataid=args.dataid,\n",
    "            multitask_heads=heads\n",
    "        )\n",
    "        savefile = find_model(args.checkpoints[cv_fold], cv_fold)\n",
    "        \n",
    "#     print('savefile', savefile)\n",
    "    ckpt = torch.load(savefile, map_location=\"cpu\")\n",
    "    \n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    for k in list(state_dict.keys()):\n",
    "        # retain only encoder_q up to before the embedding layer\n",
    "        if k.startswith('module'):\n",
    "            state_dict[k[len(\"module.\"):]] = state_dict[k] \n",
    "            del state_dict[k]\n",
    "\n",
    "                \n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "m = load_model(0, args.loss)    \n",
    "\n",
    "def rotate_images(images):\n",
    "    nimages = images.shape[0]\n",
    "    n_rot_images = 4*nimages\n",
    "\n",
    "    # rotate images all 4 ways at once\n",
    "    rotated_images = torch.zeros([n_rot_images, images.shape[1], images.shape[2], images.shape[3]]).cuda()\n",
    "    rot_classes = torch.zeros([n_rot_images]).long().cuda()\n",
    "\n",
    "    rotated_images[:nimages] = images\n",
    "    # rotate 90\n",
    "    rotated_images[nimages:2*nimages] = images.flip(3).transpose(2,3)\n",
    "    rot_classes[nimages:2*nimages] = 1\n",
    "    # rotate 180\n",
    "    rotated_images[2*nimages:3*nimages] = images.flip(3).flip(2)\n",
    "    rot_classes[2*nimages:3*nimages] = 2\n",
    "    # rotate 270\n",
    "    rotated_images[3*nimages:4*nimages] = images.transpose(2,3).flip(3)\n",
    "    rot_classes[3*nimages:4*nimages] = 3\n",
    "\n",
    "    return rotated_images, rot_classes\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def eval_augmentations(config): \n",
    "    augment = config\n",
    "    print('called', augment)\n",
    "    \n",
    "    if args.loss == 'icl_and_rotation':\n",
    "        \n",
    "        losses = ['icl', 'rotation']\n",
    "        \n",
    "    else: \n",
    "        losses = [args.loss]\n",
    "        \n",
    "    \n",
    "    metrics = Accumulator()\n",
    "    \n",
    "    for loss_type in losses: \n",
    "        # TODO MOve this out\n",
    "#         metrics = Accumulator()\n",
    "      \n",
    "        print(loss_type)\n",
    "        \n",
    "        augmentations = policy_decoder(augment, augment['num_policy'], augment['num_op'])\n",
    "        \n",
    "        \n",
    "        fold = augment['cv_fold']\n",
    "        model = load_model(cv_fold, loss_type).cuda()\n",
    "        model.eval()\n",
    "        loaders = []\n",
    "\n",
    "        for _ in range(args.num_policy): #TODO: \n",
    "            _, validloader = get_dataloaders(augmentations, 512, kfold=fold, loss_type=loss_type)\n",
    "            loaders.append(iter(validloader))\n",
    "            del _\n",
    "\n",
    "     \n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        try: \n",
    "\n",
    "            i = 0\n",
    "            with torch.no_grad(): \n",
    "                while True: \n",
    "                    losses = []\n",
    "                    corrects = []\n",
    "\n",
    "                    for loader in loaders:\n",
    "\n",
    "                        if not loss_type == 'icl':\n",
    "\n",
    "                            data, label = next(loader)\n",
    "                            data = data.cuda()\n",
    "                            label = label.cuda()\n",
    "\n",
    "                            if loss_type == 'supervised':\n",
    "                                pred = model(data)\n",
    "\n",
    "                            if loss_type ==\"rotation\":\n",
    "                                rotated_images, label = rotate_images(data)\n",
    "                                pred = model(rotated_images)  \n",
    "\n",
    "                        else: \n",
    "\n",
    "                            images, _ = next(loader)\n",
    "                            images[0] = images[0].cuda(non_blocking=True)\n",
    "                            images[1] = images[1].cuda(non_blocking=True)\n",
    "                            pred, label =model(head=\"moco\", im_q=images[0], im_k=images[1], evaluate=True)\n",
    "                            acc = accuracy(pred, label)\n",
    "\n",
    "                        loss = loss_fn(pred, label)\n",
    "                        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                        _, pred = pred.topk(1, 1, True, True)\n",
    "                        pred = pred.t()\n",
    "                        correct = pred.eq(label.view(1, -1).expand_as(pred)).detach().cpu().numpy()\n",
    "                        corrects.append(correct)\n",
    "\n",
    "                        if not loss_type == 'icl':\n",
    "                            del loss, correct, pred, data, label\n",
    "                        else: \n",
    "                            del loss, images, pred, label, correct\n",
    "\n",
    "\n",
    "\n",
    "                    losses = np.concatenate(losses)\n",
    "#                     print('losses shape' , losses.shape)\n",
    "                    losses_min = np.mean(losses) # get it so it averages out.\n",
    "                    corrects = np.concatenate(corrects)\n",
    "                    corrects_max = np.max(corrects, axis=0).squeeze()\n",
    "#                     print('len corrects max', len(corrects_max), 'corrects shape', corrects.shape)\n",
    "                    \n",
    "                    print('l shape', losses.shape)\n",
    "                    print('corrects shape', corrects.shape)\n",
    "                    print('len c max', len(corrects_max))\n",
    "                    losses_min *= losses.shape[0]/5\n",
    "                    \n",
    "                    if loss_type == 'rotation': \n",
    "                        metrics.add_dict({ \n",
    "                            'minus_loss': -.25*np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'plus_loss': np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'correct': np.sum(corrects_max)*args.loss_weights[loss_type],\n",
    "                            'cnt': len(corrects_max)})\n",
    "                        del corrects, corrects_max\n",
    "                    else: \n",
    "                        metrics.add_dict({ \n",
    "                            'minus_loss': -1*np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'plus_loss': np.sum(losses_min)*args.loss_weights[loss_type],\n",
    "                            'correct': np.sum(corrects_max)*args.loss_weights[loss_type],\n",
    "                            'cnt': len(corrects_max)})\n",
    "                        del corrects, corrects_max\n",
    "\n",
    "                    print('correct', metrics['correct']/metrics['cnt'])\n",
    "        except StopIteration: \n",
    "            pass\n",
    "    \n",
    "    del model\n",
    "    metrics = metrics/'cnt'\n",
    "    tune.track.log(top_1_valid=metrics['correct'], minus_loss=metrics['minus_loss'], plus_loss=metrics['plus_loss'])\n",
    "    print(metrics['correct'])\n",
    "    return metrics['minus_loss']\n",
    "\n",
    "ops = augment_list(False) # Get the default augmentation set. \n",
    "# Define the space of our augmentations. \n",
    "space = {}\n",
    "for i in range(args.num_policy): \n",
    "    for j in range(args.num_op):\n",
    "        space['policy_%d_%d' %(i,j)]  = hp.choice('policy_%d_%d' %(i, j), list(range(0, len(ops))))\n",
    "        space['prob_%d_%d' %(i, j)] = hp.uniform('prob_%d_%d' %(i, j), 0.0, 1.0)\n",
    "        space['level_%d_%d' %(i, j)] = hp.uniform('level_%d_%d' %(i, j), 0.0, 1.0)\n",
    "\n",
    "final_policy_set = []\n",
    "\n",
    "if not args.loss == 'icl': \n",
    "    reward_attr = 'minus_loss'\n",
    "else: \n",
    "    reward_attr = 'minus_loss'\n",
    "    \n",
    "    \n",
    "# TODO: let this be whatever we want. \n",
    "object_store_memory = int(0.6 * ray.utils.get_system_memory() // 10 ** 9 * 10 ** 9)\n",
    "\n",
    "# TODO Change back\n",
    "ray.init(num_gpus=2, ignore_reinit_error=True, \n",
    "    num_cpus=28\n",
    "    )\n",
    "# ray.init(num_gpus=1, memory=200*1024*1024*100, object_store_memory=200*1024*1024*50)\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "cv_num = 5\n",
    "num_result_per_cv = 10\n",
    "\n",
    "for _ in range(2): \n",
    "    for cv_fold in range(cv_num): \n",
    "        name = \"slm_moco_min_debug_ICL_rot_search_%s_fold_%d\" %(args.dataid, cv_fold)\n",
    "        hyperopt_search=HyperOptSearch(space, \n",
    "            max_concurrent=1,\n",
    "            metric=reward_attr,\n",
    "            mode='max')\n",
    "\n",
    "\n",
    "        results = tune.run(\n",
    "            eval_augmentations,\n",
    "            name=name,\n",
    "            num_samples=200,\n",
    "            resources_per_trial={\n",
    "                \"gpu\": 1\n",
    "            },\n",
    "            search_alg=hyperopt_search,\n",
    "            verbose=2,\n",
    "            config = { \n",
    "                'num_op': args.num_op, \n",
    "                'num_policy': args.num_policy, \n",
    "                'cv_fold': cv_fold\n",
    "            },\n",
    "            return_trials=True,\n",
    "            stop={'training_iteration': 1},\n",
    "        )\n",
    "        results_copy = results\n",
    "        results = [x for x in results if x.last_result is not None]\n",
    "        results = sorted(results, key= lambda x: x.last_result[reward_attr], reverse=True)\n",
    "\n",
    "        for result in results[:num_result_per_cv]: \n",
    "            final_policy = policy_decoder(result.config, args.num_policy, args.num_op)\n",
    "            final_policy_set.extend(final_policy)\n",
    "\n",
    "        print(final_policy)\n",
    "print(final_policy_set)\n",
    "\n",
    "# Start saving to a path called policies. \n",
    "import pickle\n",
    "savefp = os.path.join(args.policy_dir, str(args.base+'_'+args.loss+ '_' + (str(reward_attr) + '.pkl')))\n",
    "with open(savefp, 'wb') as f: \n",
    "    pickle.dump(final_policy_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
